{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "809370be-7275-4592-8d2e-69b2b852aa88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Connect to NY511.ORG API to download road events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bee4cfc-24a0-400f-946b-15c4a8807c80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType, IntegerType, BooleanType\n",
    "\n",
    "# Define the Schedule schema (Array of Structs)\n",
    "schedule_schema = ArrayType(StructType([\n",
    "    StructField(\"ScheduleId\", IntegerType(), True),\n",
    "    StructField(\"Start\", StringType(), True),\n",
    "    StructField(\"End\", StringType(), True),\n",
    "    StructField(\"Continuous\", BooleanType(), True),\n",
    "    StructField(\"ActiveDays\", ArrayType(StringType()), True),\n",
    "    StructField(\"Impact\", StringType(), True)  # Impact field added\n",
    "]))\n",
    "\n",
    "# Define the Main Schema\n",
    "schema = StructType([\n",
    "    StructField(\"LastUpdated\", StringType(), True),\n",
    "    StructField(\"Latitude\", DoubleType(), True),\n",
    "    StructField(\"Longitude\", DoubleType(), True),\n",
    "    StructField(\"PlannedEndDate\", StringType(), True),\n",
    "    StructField(\"Reported\", StringType(), True),\n",
    "    StructField(\"StartDate\", StringType(), True),\n",
    "    StructField(\"ID\", StringType(), True),\n",
    "    StructField(\"RegionName\", StringType(), True),\n",
    "    StructField(\"CountyName\", StringType(), True),\n",
    "    StructField(\"Severity\", StringType(), True),\n",
    "    StructField(\"RoadwayName\", StringType(), True),\n",
    "    StructField(\"DirectionOfTravel\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"Location\", StringType(), True),\n",
    "    StructField(\"LanesAffected\", StringType(), True),\n",
    "    StructField(\"LanesStatus\", StringType(), True),\n",
    "    StructField(\"LcsEntries\", ArrayType(StructType([])), True),  # Assuming empty object array\n",
    "    StructField(\"NavteqLinkId\", StringType(), True),\n",
    "    StructField(\"PrimaryLocation\", StringType(), True),\n",
    "    StructField(\"SecondaryLocation\", StringType(), True),\n",
    "    StructField(\"FirstArticleCity\", StringType(), True),\n",
    "    StructField(\"SecondCity\", StringType(), True),\n",
    "    StructField(\"EventType\", StringType(), True),\n",
    "    StructField(\"EventSubType\", StringType(), True),\n",
    "    StructField(\"MapEncodedPolyline\", StringType(), True),\n",
    "    StructField(\"Schedule\", schedule_schema, True)  # Corrected to ArrayType(StructType([...]))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "326b168e-90b0-4d36-a88e-801ef52a9e70",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "filterBlob": "{\"filterGroups\":[],\"syncTimestamp\":1741784254929}",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"aW1wb3J0IHJlcXVlc3RzCmltcG9ydCBkYXRldGltZQpmcm9tIHB5c3Bhcmsuc3FsLmZ1bmN0aW9ucyBpbXBvcnQgdG9fdGltZXN0YW1wLCBjdXJyZW50X3RpbWVzdGFtcAoKIyBBUEkgcmVxdWVzdCBVUkwKQVBJX0tFWSA9ICIzODc2Yjg3NDUzZmI0YzdjODJmNzFkODlhZGUzZGUzNiIKVVJMID0gZiJodHRwczovLzUxMW55Lm9yZy9hcGkvZ2V0ZXZlbnRzP2tleT17QVBJX0tFWX0mZm9ybWF0PWpzb24iCgojIE1ha2UgdGhlIHJlcXVlc3QKcmVzcG9uc2UgPSByZXF1ZXN0cy5nZXQoVVJMKQpkYXRhID0gcmVzcG9uc2UuanNvbigpCgojIENyZWF0ZSBTcGFyayBEYXRhRnJhbWUKZGYgPSBzcGFyay5jcmVhdGVEYXRhRnJhbWUoZGF0YSwgc2NoZW1hPXNjaGVtYSkKZGYgPSBkZi53aXRoQ29sdW1uKCJSZXBvcnRlZERhdGUiLCB0b190aW1lc3RhbXAoZGZbIlJlcG9ydGVkIl0sImRkL01NL3l5eXkgSEg6bW06c3MiKSkuZHJvcCgiUmVwb3J0ZWQiKQoKIyBEaXNwbGF5IERhdGFGcmFtZQpkaXNwbGF5KGRmKQo=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView2e6d63f\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView2e6d63f\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView2e6d63f\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView2e6d63f) SELECT DATE_TRUNC('MONTH',`ReportedDate`) `column_fdbc330b28`,COUNT(*) `column_fdbc330b25` FROM q GROUP BY `column_fdbc330b28`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView2e6d63f\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "ReportedDate",
             "id": "column_fdbc330b28",
             "transform": "MONTH_LEVEL"
            },
            "y": [
             {
              "column": "*",
              "id": "column_fdbc330b25",
              "transform": "COUNT"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_fdbc330b25": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "92e2d88f-a6c8-4e38-a70d-f8ad8f511a0a",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 11,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "column_fdbc330b28",
           "type": "column"
          }
         ],
         "selects": [
          {
           "alias": "column_fdbc330b28",
           "args": [
            {
             "column": "ReportedDate",
             "type": "column"
            },
            {
             "string": "MONTH",
             "type": "string"
            }
           ],
           "function": "DATE_TRUNC",
           "type": "function"
          },
          {
           "alias": "column_fdbc330b25",
           "args": [
            {
             "column": "*",
             "type": "column"
            }
           ],
           "function": "COUNT",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import datetime\n",
    "from pyspark.sql.functions import to_timestamp, current_timestamp\n",
    "\n",
    "# API request URL\n",
    "API_KEY = \"\"\n",
    "URL = f\"https://511ny.org/api/getevents?key={API_KEY}&format=json\"\n",
    "\n",
    "# Make the request\n",
    "response = requests.get(URL)\n",
    "data = response.json()\n",
    "\n",
    "# Create Spark DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df = df.withColumn(\"ReportedDate\", to_timestamp(df[\"Reported\"],\"dd/MM/yyyy HH:mm:ss\")).drop(\"Reported\")\n",
    "\n",
    "# Display DataFrame\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Historical Weather Data for Road Collision using openmeteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openmeteo-requests requests-cache retry-requests numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import openmeteo_requests\n",
    "\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "from retry_requests import retry\n",
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = -1)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "# Make sure all required weather variables are listed here\n",
    "# The order of variables in hourly or daily is important to assign them correctly below\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "for d in weather_dates_df:\n",
    "  collision_data = spark.sql(f\"select distinct crash_date, round(longitude,1) longitude , round(latitude,1) latitude from {catalog_name}.{schema_name}.collision_bronze where year(crash_date) = 2024 and longitude is not null and latitude is not null and round(longitude,1) != 0.0 and round(latitude,1) != 0.0 and crash_date = '{d.crash_date}'\").collect()\n",
    "  \n",
    "  weather_df = pd.DataFrame()\n",
    "  c = 0\n",
    "\n",
    "  for r in collision_data:\n",
    "     params = {\n",
    "     \"latitude\": r.latitude,\n",
    "\t   \"longitude\": r.longitude,\n",
    "\t   \"start_date\": r.crash_date,\n",
    "\t   \"end_date\": r.crash_date,\n",
    "\t   \"hourly\": [\"temperature_2m\", \"precipitation\", \"rain\", \"snowfall\", \"snow_depth\", \"weather_code\", \"wind_speed_10m\", \"wind_direction_10m\", \"wind_gusts_10m\"],\n",
    "\t   \"temperature_unit\": \"fahrenheit\",\n",
    "\t   \"wind_speed_unit\": \"mph\",\n",
    "\t   \"precipitation_unit\": \"inch\",\n",
    "\t   \"timezone\": \"America/New_York\"\n",
    "     }\n",
    "     try:\n",
    "       responses = openmeteo.weather_api(url, params=params)\n",
    "     except Exception as e:\n",
    "       if \"Hourly API request limit exceeded. Please try again in the next hour.\" in str(e):\n",
    "         time.sleep(3600)\n",
    "       else:\n",
    "         raise e\n",
    "     # Process first location. Add a for-loop for multiple locations or weather models\n",
    "     response = responses[0]\n",
    "\n",
    "     # Process hourly data. The order of variables needs to be the same as requested.\n",
    "     hourly = response.Hourly()\n",
    "     hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "     hourly_precipitation = hourly.Variables(1).ValuesAsNumpy()\n",
    "     hourly_rain = hourly.Variables(2).ValuesAsNumpy()\n",
    "     hourly_snowfall = hourly.Variables(3).ValuesAsNumpy()\n",
    "     hourly_snow_depth = hourly.Variables(4).ValuesAsNumpy()\n",
    "     hourly_weather_code = hourly.Variables(5).ValuesAsNumpy()\n",
    "     hourly_wind_speed_10m = hourly.Variables(6).ValuesAsNumpy()\n",
    "     hourly_wind_direction_10m = hourly.Variables(7).ValuesAsNumpy()\n",
    "     hourly_wind_gusts_10m = hourly.Variables(8).ValuesAsNumpy()\n",
    "\n",
    "     hourly_data = {\"datetime\": pd.date_range(\n",
    "       start = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "\t     end = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "\t     freq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "\t     inclusive = \"left\"\n",
    "      )}\n",
    "     hourly_data[\"date\"] = r.crash_date\n",
    "     hourly_data[\"latitude\"] = r.latitude\n",
    "     hourly_data[\"longitude\"] = r.longitude\n",
    "     hourly_data[\"temperature_2m\"] = hourly_temperature_2m\n",
    "     hourly_data[\"precipitation\"] = hourly_precipitation\n",
    "     hourly_data[\"rain\"] = hourly_rain\n",
    "     hourly_data[\"snowfall\"] = hourly_snowfall\n",
    "     hourly_data[\"snow_depth\"] = hourly_snow_depth\n",
    "     hourly_data[\"weather_code\"] = hourly_weather_code\n",
    "     hourly_data[\"wind_speed_10m\"] = hourly_wind_speed_10m\n",
    "     hourly_data[\"wind_direction_10m\"] = hourly_wind_direction_10m\n",
    "     hourly_data[\"wind_gusts_10m\"] = hourly_wind_gusts_10m\n",
    "\n",
    "     hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
    "     weather_df = pd.concat([weather_df, hourly_dataframe])\n",
    "     c = c+1\n",
    "     print(c)\n",
    "  weather_sdf = spark.createDataFrame(weather_df)\n",
    "  #weather_sdf.write.mode(\"append\").saveAsTable(f\"{catalog_name}.{schema_name}.weather_history_bronze\")   "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1315887243181124,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "SolAccelerator-Auto_Geospatial",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
