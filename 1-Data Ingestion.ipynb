{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Geospatial Analytics - Data Ingestion\n",
    "\n",
    "**Mobility and Automotive - Road Safety and Risk Prevention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook delivers the ingestion pipeline to build the medallion architecture for the geospatial use case. To make it easily reproducible, it downloads raw datasets from APIs (or a repo) into UC Volumes (bronze). All subsequent steps in the pipeline then read raw files from the Volume to build the silver and gold layers.\n",
    "\n",
    "We provide sample data from public APIs in the USA, primarily NYC data sources given their availability.\n",
    "\n",
    "Customers can plug in any dataset as long as it provides the following:\n",
    "\n",
    "- **Collisions**: A set of road incidents, including contributing factors\n",
    "- **Traffic Volume**: Historical traffic volume for a set of latitude/longitude points\n",
    "- **Road Condition**: Road and traffic conditions\n",
    "- **Rides**: A set of rides/drives with distance, pick and droff locations.\n",
    "- **Weather**: Historical weather conditions for a set of latitude/longitude points\n",
    "- **Telematics**: A set of rides/drives with driving metrics like acceleration and speed\n",
    "\n",
    "\n",
    "⚙️ **Note**: Ensure to configure `./Config.ipynb` to set the destination catalog and volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Depedencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-comment the following lines to generate the requirements.txt file from requirements.in file\n",
    "# %pip install pip-tools\n",
    "# !pip-compile requirements.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "\n",
    "try:\n",
    "    dbutils.library.restartPython()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Configs and Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Design\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a medallion architecture pipeline to populate a final gold schema providing the following schema. Provided you can populate this gold schema, you should reuse much of our code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mm(\"\"\"\n",
    "erDiagram\n",
    "\n",
    "COLLISION_BRONZE {\n",
    "  STRING crash_date\n",
    "  STRING crash_time\n",
    "  STRING borough\n",
    "  STRING zip_code\n",
    "  DOUBLE latitude\n",
    "  DOUBLE longitude\n",
    "  STRING location\n",
    "  STRING on_street_name\n",
    "  STRING cross_street_name\n",
    "  STRING off_street_name\n",
    "  STRING number_of_persons_injured\n",
    "  INT number_of_persons_killed\n",
    "  INT number_of_pedestrians_injured\n",
    "  INT number_of_pedestrians_killed\n",
    "  INT number_of_cyclist_injured\n",
    "  STRING number_of_cyclist_killed\n",
    "  STRING number_of_motorist_injured\n",
    "  INT number_of_motorist_killed\n",
    "  STRING contributing_factor_vehicle_1\n",
    "  STRING contributing_factor_vehicle_2\n",
    "  STRING contributing_factor_vehicle_3\n",
    "  STRING contributing_factor_vehicle_4\n",
    "  STRING contributing_factor_vehicle_5\n",
    "  INT collision_id\n",
    "  STRING vehicle_type_code_1\n",
    "  STRING vehicle_type_code_2\n",
    "  STRING vehicle_type_code_3\n",
    "  STRING vehicle_type_code_4\n",
    "  STRING vehicle_type_code_5\n",
    "}\n",
    "\n",
    "COLLISION_SILVER {\n",
    "  DATE crash_date\n",
    "  TIMESTAMP crash_time\n",
    "  STRING borough\n",
    "  STRING zip_code\n",
    "  DOUBLE latitude\n",
    "  DOUBLE longitude\n",
    "  STRING on_street_name\n",
    "  STRING cross_street_name\n",
    "  STRING off_street_name\n",
    "  INT number_of_persons_injured\n",
    "  INT number_of_persons_killed\n",
    "  INT number_of_pedestrians_injured\n",
    "  INT number_of_pedestrians_killed\n",
    "  INT number_of_cyclist_injured\n",
    "  INT number_of_cyclist_killed\n",
    "  INT number_of_motorist_injured\n",
    "  INT number_of_motorist_killed\n",
    "  STRING contributing_factor_vehicle_1\n",
    "  STRING contributing_factor_vehicle_2\n",
    "  STRING contributing_factor_vehicle_3\n",
    "  STRING contributing_factor_vehicle_4\n",
    "  STRING contributing_factor_vehicle_5\n",
    "  INT collision_id\n",
    "  STRING vehicle_type_code_1\n",
    "  STRING vehicle_type_code_2\n",
    "  STRING vehicle_type_code_3\n",
    "  STRING vehicle_type_code_4\n",
    "  STRING vehicle_type_code_5\n",
    "}\n",
    "\n",
    "TRAFFIC_VOLUME_COUNTS_BRONZE {\n",
    "  BIGINT requestid\n",
    "  STRING boro\n",
    "  INT yr\n",
    "  INT m\n",
    "  INT d\n",
    "  INT hh\n",
    "  INT mm\n",
    "  INT vol\n",
    "  BIGINT segmentid\n",
    "  STRING wktgeom\n",
    "  STRING street\n",
    "  STRING fromst\n",
    "  STRING tost\n",
    "  STRING direction\n",
    "}\n",
    "\n",
    "TRAFFIC_VOLUME_COUNTS_SILVER {\n",
    "  INT vol\n",
    "  DOUBLE lat\n",
    "  DOUBLE long\n",
    "  TIMESTAMP timestamp\n",
    "  BIGINT h3_index\n",
    "}\n",
    "\n",
    "ROAD_CONDITION {\n",
    "  DOUBLE latitude\n",
    "  DOUBLE longitude\n",
    "  STRING id\n",
    "  STRING region_name\n",
    "  STRING county_name\n",
    "  STRING severity\n",
    "  STRING roadway_name\n",
    "  STRING direction_of_travel\n",
    "  STRING description\n",
    "  STRING lanes_affected\n",
    "  STRING lanes_status\n",
    "  STRING navteq_link_id\n",
    "  STRING primary_location\n",
    "  STRING secondary_location\n",
    "  STRING first_article_city\n",
    "  STRING second_city\n",
    "  STRING event_type\n",
    "  STRING event_sub_type\n",
    "  TIMESTAMP reported_date\n",
    "  TIMESTAMP start_date\n",
    "  TIMESTAMP planned_end_date\n",
    "}\n",
    "\n",
    "TELEMATICS_BRONZE {\n",
    "  BIGINT vehicle_id\n",
    "  STRING vin_number\n",
    "  TIMESTAMP timestamp\n",
    "  DOUBLE latitude\n",
    "  DOUBLE longitude\n",
    "  BOOLEAN abs\n",
    "  BOOLEAN tpms\n",
    "  BOOLEAN brake_pad\n",
    "  BOOLEAN accelerometer\n",
    "  BOOLEAN blind_spot\n",
    "  BOOLEAN lane_departure\n",
    "  BOOLEAN fcw\n",
    "  BOOLEAN parking_sensor\n",
    "  BOOLEAN rear_view_camera\n",
    "  BOOLEAN engine_light\n",
    "  BOOLEAN rain_sensor\n",
    "  BOOLEAN wiper_blades\n",
    "  BOOLEAN fog_light\n",
    "  BOOLEAN seatbelt_off\n",
    "  BOOLEAN door_ajar\n",
    "  DOUBLE total_fuel_consumption\n",
    "  DOUBLE fuel_level_percent\n",
    "  DOUBLE fuel_level_liters\n",
    "  DOUBLE vehicle_mileage_dashboard\n",
    "  DOUBLE vehicle_mileage_computed\n",
    "  INT engine_speed_rpm\n",
    "  BOOLEAN oil_level_status\n",
    "  DOUBLE engine_temperature\n",
    "  DOUBLE vehicle_speed\n",
    "  DOUBLE accelerator_pedal_position\n",
    "  BOOLEAN lights_parking\n",
    "  BOOLEAN lights_dipped\n",
    "  BOOLEAN lights_full_beam\n",
    "  BOOLEAN lights_fog_front\n",
    "  BOOLEAN lights_fog_rear\n",
    "  BOOLEAN door_front_left\n",
    "  BOOLEAN door_front_right\n",
    "  BOOLEAN door_rear_right\n",
    "  BOOLEAN door_rear_left\n",
    "  BOOLEAN trunk_cover\n",
    "  BOOLEAN engine_hood\n",
    "  BOOLEAN charging_cable_status\n",
    "  BOOLEAN charging_status\n",
    "  DOUBLE battery_level\n",
    "  DOUBLE vehicle_range_km\n",
    "  DOUBLE battery_temperature\n",
    "  DOUBLE cng_level\n",
    "  DOUBLE total_cng_consumption\n",
    "  BOOLEAN engine_on_cng\n",
    "  DOUBLE adblue_level_dashboard\n",
    "  BOOLEAN seatbelt_driver\n",
    "  BOOLEAN seatbelt_passenger\n",
    "  BOOLEAN check_engine\n",
    "  BOOLEAN lights_failure\n",
    "  BOOLEAN low_tire_pressure\n",
    "}\n",
    "\n",
    "TRIPS_BRONZE {\n",
    "  TIMESTAMP tpep_pickup_datetime\n",
    "  TIMESTAMP tpep_dropoff_datetime\n",
    "  DOUBLE trip_distance\n",
    "  DOUBLE fare_amount\n",
    "  INT pickup_zip\n",
    "  INT dropoff_zip\n",
    "  STRING trip_type\n",
    "}\n",
    "\n",
    "TRIPS_SILVER {\n",
    "  TIMESTAMP tpep_pickup_datetime\n",
    "  TIMESTAMP tpep_dropoff_datetime\n",
    "  DOUBLE trip_distance\n",
    "  INT pickup_zip\n",
    "  INT dropoff_zip\n",
    "  DOUBLE pickup_latitude\n",
    "  DOUBLE pickup_longitude\n",
    "  DOUBLE dropoff_latitude\n",
    "  DOUBLE dropoff_longitude\n",
    "  STRING trip_type\n",
    "}\n",
    "\n",
    "WEATHER_HISTORY_BRONZE {\n",
    "  TIMESTAMP datetime\n",
    "  DATE date\n",
    "  DOUBLE latitude\n",
    "  DOUBLE longitude\n",
    "  FLOAT temperature_2m\n",
    "  FLOAT precipitation\n",
    "  FLOAT rain\n",
    "  FLOAT snowfall\n",
    "  FLOAT snow_depth\n",
    "  FLOAT weather_code\n",
    "  FLOAT wind_speed_10m\n",
    "  FLOAT wind_direction_10m\n",
    "  FLOAT wind_gusts_10m\n",
    "}\n",
    "\n",
    "COLLISION_BRONZE ||--|| COLLISION_SILVER : collision_id\n",
    "TRAFFIC_VOLUME_COUNTS_BRONZE ||--|| TRAFFIC_VOLUME_COUNTS_SILVER : request_id\n",
    "TRIPS_BRONZE ||--o| TRIPS_SILVER : maps_to\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public datasets used for this example pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "| Dataset       | Source Used                                                                                  | Description                                       | Note                                                        |\n",
    "|----------------|---------------------------------------------------------------------------------------------|---------------------------------------------------|-------------------------------------------------------------|\n",
    "| Collisions     | [NYC Open Data](http://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95/about_data) | A set of road incidents, including contributing factors. | Refer to [Terms of Use](https://opendata.cityofnewyork.us/overview/#termsofuse) for more information.                                    |\n",
    "| Traffic Volume | [NYC Open Data](https://data.cityofnewyork.us/Transportation/Automated-Traffic-Volume-Counts/7ym2-wayt/about_data) | Historical traffic volume for a set of lat/long.   | Refer to [Terms of Use](https://opendata.cityofnewyork.us/overview/#termsofuse) for more information.                                  |\n",
    "| Road Condition | [511ny](https://511ny.org)                                                                  | Road and traffic conditions.                       | Refer to [Developer Access Agreement](https://511ny.org/developers/daa) for more information                  |\n",
    "| Weather        | [Open-Meteo Weather API](http://pypi.org/project/openmeteo-requests/)                        | Historical weather data.                           | Licensed under [Attribution 4.0 International (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)      |\n",
    "| Telematics     | [Synthetic](source)                                                                         | A set of rides/drives with driving metrics like acceleration, speed. | Uses [dbldatagen](https://github.com/databrickslabs/dbldatagen) for synthetic generation. |\n",
    "\n",
    "Please view [./1A-Data Download.ipynb](./1A-Data%20Download.ipynb) to understand how the Weather and Road Condition CSV datasets were created.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Upload Sample Datasets into UC Volume\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collisions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"./data/Motor_Vehicle_Collisions_-_Crashes_20250328.csv\"\n",
    "\n",
    "\n",
    "volume_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/data/collisions\"\n",
    "\n",
    "upload_file_to_volume(local_path, volume_path, \"Motor_Vehicle_Collisions_-_Crashes_20250328.csv\"  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Traffic Volume** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "local_path = \"./data/Automated_Traffic_Volume_Counts_20240903.csv.gz\"\n",
    "\n",
    "# Destination in the Volume\n",
    "volume_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/data/traffic_volume\"\n",
    "\n",
    "upload_file_to_volume(local_path, volume_path, \"Automated_Traffic_Volume_Counts_20240903.csv.gz\"  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Road Condition and Events**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"./data/road_condition_2024.csv.gz\"\n",
    "\n",
    "# Destination in the Volume\n",
    "volume_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/data/road_condition\"\n",
    "\n",
    "upload_file_to_volume(local_path, volume_path, \"road_condition_2024.csv.gz\"  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contributing Factors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "local_path = \"./data/contributing_factor_tags_lkp_gold.csv\"\n",
    "\n",
    "# Destination in the Volume\n",
    "volume_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/data/contributing_factor\"\n",
    "\n",
    "upload_file_to_volume(local_path, volume_path, \"contributing_factor_tags_lkp_gold.csv\"  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weather - Contains weather condition for all 2024 crashes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"./data/historical_weather_crashes_2024.csv\"\n",
    "\n",
    "# Destination in the Volume\n",
    "volume_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/data/weather\"\n",
    "\n",
    "upload_file_to_volume(local_path, volume_path, \"historical_weather_crashes.csv\"  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Create Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_ddl = f'''\n",
    "CREATE OR REPLACE TABLE {catalog_name}.{schema_name}.collision_bronze (\n",
    "  crash_date STRING COMMENT 'Represents the date when the collision occurred.',\n",
    "  crash_time STRING COMMENT 'Represents the time when the collision occurred.',\n",
    "  borough STRING COMMENT 'Represents the borough where the collision took place.',\n",
    "  zip_code STRING COMMENT 'Represents the zip code where the collision took place.',\n",
    "  latitude DOUBLE COMMENT 'Represents the latitude of the collision location.',\n",
    "  longitude DOUBLE COMMENT 'Represents the longitude of the collision location.',\n",
    "  location STRING COMMENT 'Represents the human-readable description of the collision location.',\n",
    "  on_street_name STRING COMMENT 'Represents the name of the street where the collision took place.',\n",
    "  cross_street_name STRING COMMENT 'Represents the name of the cross street where the collision took place.',\n",
    "  off_street_name STRING COMMENT 'Represents the name of the off-street location where the collision took place.',\n",
    "  number_of_persons_injured STRING COMMENT 'Represents the number of people who were injured in the collision.',\n",
    "  number_of_persons_killed INT COMMENT 'Represents the number of people who were killed in the collision.',\n",
    "  number_of_pedestrians_injured INT COMMENT 'Represents the number of pedestrians who were injured in the collision.',\n",
    "  number_of_pedestrians_killed INT COMMENT 'Represents the number of pedestrians who were killed in the collision.',\n",
    "  number_of_cyclist_injured INT COMMENT 'Represents the number of cyclists who were injured in the collision.',\n",
    "  number_of_cyclist_killed STRING COMMENT 'Represents the number of cyclists who were killed in the collision.',\n",
    "  number_of_motorist_injured STRING COMMENT 'Represents the number of motorists who were injured in the collision.',\n",
    "  number_of_motorist_killed INT COMMENT 'Represents the number of motorists who were killed in the collision.',\n",
    "  contributing_factor_vehicle_1 STRING COMMENT 'Represents the first contributing factor of the collision, related to one of the vehicles involved.',\n",
    "  contributing_factor_vehicle_2 STRING COMMENT 'Represents the second contributing factor of the collision, related to one of the vehicles involved.',\n",
    "  contributing_factor_vehicle_3 STRING COMMENT 'Represents the third contributing factor of the collision, if applicable.',\n",
    "  contributing_factor_vehicle_4 STRING COMMENT 'Represents the fourth contributing factor of the collision, if applicable.',\n",
    "  contributing_factor_vehicle_5 STRING COMMENT 'Represents the fifth contributing factor of the collision, if applicable.',\n",
    "  collision_id INT COMMENT 'Unique identifier for the collision, allowing easy reference and tracking.',\n",
    "  vehicle_type_code_1 STRING COMMENT 'First vehicle type code associated with the collision.',\n",
    "  vehicle_type_code_2 STRING COMMENT 'Second vehicle type code associated with the collision.',\n",
    "  vehicle_type_code_3 STRING COMMENT 'Third vehicle type code associated with the collision.',\n",
    "  vehicle_type_code_4 STRING COMMENT 'Fourth vehicle type code associated with the collision.',\n",
    "  vehicle_type_code_5 STRING COMMENT 'Fifth vehicle type code associated with the collision.'\n",
    ")\n",
    "COMMENT 'This table contains data on traffic collisions. It includes details such as the date, time, location, and contributing factors. This data can be used to analyze traffic patterns, identify high-risk areas, and understand the causes of accidents. It can also help in planning traffic management strategies, allocating resources, and improving road safety.'\n",
    "'''\n",
    "\n",
    "spark.sql(sql_ddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_ddl = f'''\n",
    "CREATE OR REPLACE TABLE {catalog_name}.{schema_name}.collision_silver (\n",
    "  crash_date DATE COMMENT 'Represents the date of the collision.',\n",
    "  crash_time TIMESTAMP COMMENT 'Represents the time of the collision.',\n",
    "  borough STRING COMMENT 'Identifies the borough where the collision occurred.',\n",
    "  zip_code STRING COMMENT 'Represents the zip code where the collision took place.',\n",
    "  latitude DOUBLE COMMENT 'Represents the latitude of the collision location.',\n",
    "  longitude DOUBLE COMMENT 'Represents the longitude of the collision location.',\n",
    "  on_street_name STRING COMMENT 'Identifies the name of the street where the collision occurred.',\n",
    "  cross_street_name STRING COMMENT 'Represents the name of the cross street where the collision took place.',\n",
    "  off_street_name STRING COMMENT 'Identifies the name of the off-street location where the collision occurred.',\n",
    "  number_of_persons_injured INT COMMENT 'Represents the number of people who were injured in the collision.',\n",
    "  number_of_persons_killed INT COMMENT 'Represents the number of people who were killed in the collision.',\n",
    "  number_of_pedestrians_injured INT COMMENT 'Identifies the number of pedestrians who were injured in the collision.',\n",
    "  number_of_pedestrians_killed INT COMMENT 'Represents the number of pedestrians who were killed in the collision.',\n",
    "  number_of_cyclist_injured INT COMMENT 'Identifies the number of cyclists who were injured in the collision.',\n",
    "  number_of_cyclist_killed INT COMMENT 'Represents the number of cyclists who were killed in the collision.',\n",
    "  number_of_motorist_injured INT COMMENT 'Identifies the number of motorists who were injured in the collision.',\n",
    "  number_of_motorist_killed INT COMMENT 'Represents the number of motorists who were killed in the collision.',\n",
    "  contributing_factor_vehicle_1 STRING COMMENT 'Represents the first contributing factor of the collision involving a vehicle.',\n",
    "  contributing_factor_vehicle_2 STRING COMMENT 'Identifies the second contributing factor of the collision involving a vehicle.',\n",
    "  contributing_factor_vehicle_3 STRING COMMENT 'Represents the third contributing factor of the collision involving a vehicle.',\n",
    "  contributing_factor_vehicle_4 STRING COMMENT 'Represents the fourth contributing factor of the collision, if applicable.',\n",
    "  contributing_factor_vehicle_5 STRING COMMENT 'Represents the fifth contributing factor of the collision, if applicable.',\n",
    "  collision_id INT COMMENT 'Unique identifier for the collision.',\n",
    "  vehicle_type_code_1 STRING COMMENT 'Represents the first vehicle type code involved in the collision.',\n",
    "  vehicle_type_code_2 STRING COMMENT 'Represents the second vehicle type code involved in the collision.',\n",
    "  vehicle_type_code_3 STRING COMMENT 'Represents the third vehicle type code involved in the collision.',\n",
    "  vehicle_type_code_4 STRING COMMENT 'Represents the fourth vehicle type code involved in the collision.',\n",
    "  vehicle_type_code_5 STRING COMMENT 'Represents the fifth vehicle type code involved in the collision.'\n",
    ")\n",
    "COMMENT 'The table contains data related to traffic accidents. It includes details such as the date and time of the accident, the location (including borough, zip code, latitude, and longitude), and the number of people involved (pedestrians, cyclists, and motorists). The table also captures the contributing factors of the accidents. This data can be used to analyze traffic patterns, identify accident-prone areas, and assess the impact of various factors on accidents. It can also help in developing strategies to reduce accidents and improve road safety in the city.'\n",
    "'''\n",
    "\n",
    "spark.sql(sql_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traffic Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "sql_ddl = f'''\n",
    "CREATE OR REPLACE TABLE {catalog_name}.{schema_name}.traffic_volume_counts_bronze (\n",
    "  requestid BIGINT COMMENT 'A unique ID that is generated for each counts request.',\n",
    "  boro STRING COMMENT 'Lists which of the five administrative divisions of New York City the location is within, written as a word.',\n",
    "  yr INT COMMENT 'The two-digit year portion of the date when the count was conducted.',\n",
    "  m INT COMMENT 'The two-digit month portion of the date when the count was conducted.',\n",
    "  d INT COMMENT 'The two-digit day portion of the date when the count was conducted.',\n",
    "  hh INT COMMENT 'The two-digit hour portion of the time when the count was conducted.',\n",
    "  mm INT COMMENT 'The two-digit start minute portion of the time when the count was conducted.',\n",
    "  vol INT COMMENT 'The total sum of counts collected within a 15-minute increment.',\n",
    "  segmentid BIGINT COMMENT 'The ID that identifies each segment of a street in the LION street network version 14.',\n",
    "  wktgeom STRING COMMENT 'A text markup language for representing vector geometry objects on a map and spatial reference systems of spatial objects.',\n",
    "  street STRING COMMENT 'The On Street where the count took place.',\n",
    "  fromst STRING COMMENT 'The From Street where the count took place.',\n",
    "  tost STRING COMMENT 'The To Street where the count took place.',\n",
    "  direction STRING COMMENT 'The text-based direction of traffic where the count took place.'\n",
    ")\n",
    "COMMENT 'Traffic volume count data. Schema based on https://data.cityofnewyork.us/Transportation/Automated-Traffic-Volume-Counts/7ym2-wayt/about_data.';\n",
    "'''\n",
    "\n",
    "spark.sql(sql_ddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog_name}.{schema_name}.traffic_volume_counts_silver (\n",
    "  vol INT COMMENT 'The total sum of counts collected within a 15-minute increment.',\n",
    "  lat DOUBLE COMMENT 'Latitude derived from geometry.',\n",
    "  long DOUBLE COMMENT 'Longitude derived from geometry.',\n",
    "  timestamp TIMESTAMP COMMENT 'Timestamp of the count event.',\n",
    "  h3_index BIGINT COMMENT 'H3 index (resolution 9) based on lat/long.'\n",
    ")\n",
    "CLUSTER BY (h3_index)\n",
    "COMMENT 'Traffic volume count data with lat/log and H3 index.';\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Road Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog_name}.{schema_name}.road_condition_bronze (\n",
    "    latitude DOUBLE COMMENT 'Latitude coordinate of the road event.',\n",
    "    longitude DOUBLE COMMENT 'Longitude coordinate of the road event.',\n",
    "    id STRING COMMENT 'Unique identifier for the road condition event.',\n",
    "    region_name STRING COMMENT 'Name of the region where the event occurred.',\n",
    "    county_name STRING COMMENT 'County in which the road event was reported.',\n",
    "    severity STRING COMMENT 'Severity level of the road condition or event.',\n",
    "    roadway_name STRING COMMENT 'Name of the affected roadway.',\n",
    "    direction_of_travel STRING COMMENT 'Direction of travel on the affected roadway.',\n",
    "    description STRING COMMENT 'Description of the road event or condition.',\n",
    "    lanes_affected STRING COMMENT 'Information about lanes impacted by the event.',\n",
    "    lanes_status STRING COMMENT 'Current status of affected lanes (e.g., closed, open, restricted).',\n",
    "    navteq_link_id STRING COMMENT 'Navteq link ID used for mapping the roadway segment.',\n",
    "    primary_location STRING COMMENT 'Primary location detail for the road event.',\n",
    "    secondary_location STRING COMMENT 'Secondary location reference related to the event.',\n",
    "    first_article_city STRING COMMENT 'Primary city associated with the road event.',\n",
    "    second_city STRING COMMENT 'Secondary city or neighboring city reference.',\n",
    "    event_type STRING COMMENT 'General type of road event (e.g., construction, incident, closure).',\n",
    "    event_sub_type STRING COMMENT 'More specific classification of the road event.',\n",
    "    reported_date TIMESTAMP COMMENT 'Timestamp when the event was first reported.',\n",
    "    start_date TIMESTAMP COMMENT 'Planned or actual start date of the event.',\n",
    "    planned_end_date TIMESTAMP COMMENT 'Planned end date of the road event.'\n",
    ")\n",
    "COMMENT 'This table contains geospatial road condition data, including planned events such as closures, construction, or hazards. Each record captures key location, time, and severity details to support mapping, traffic analysis, and operational decision-making.';\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog_name}.{schema_name}.trips_bronze (\n",
    "  tpep_pickup_datetime TIMESTAMP COMMENT 'Timestamp when the passenger was picked up.',\n",
    "  tpep_dropoff_datetime TIMESTAMP COMMENT 'Timestamp when the passenger was dropped off.',\n",
    "  trip_distance DOUBLE COMMENT 'Total distance of the trip in miles.',\n",
    "  fare_amount DOUBLE COMMENT 'Fare charged for the trip in USD.',\n",
    "  pickup_zip INT COMMENT 'ZIP code of the pickup location.',\n",
    "  dropoff_zip INT COMMENT 'ZIP code of the dropoff location.',\n",
    "  trip_type STRING COMMENT 'Type of trip (e.g., Taxi, Ride Sharing).'\n",
    ")\n",
    "COMMENT 'Contains car trips. Raw trip data including pickup/dropoff times, trip distance, fare amount, and ZIP codes for pickup and dropoff locations.';\"\"\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog_name}.{schema_name}.trips_silver (\n",
    "  id BIGINT GENERATED ALWAYS AS IDENTITY COMMENT 'Auto populated unique identifier for the trip.',\n",
    "  tpep_pickup_datetime TIMESTAMP COMMENT 'The timestamp of the taxi trip pickup.',\n",
    "  tpep_dropoff_datetime TIMESTAMP COMMENT 'The timestamp of the taxi trip dropoff.',\n",
    "  trip_distance DOUBLE COMMENT 'The distance of the taxi trip, measured in miles.',\n",
    "  pickup_zip INT COMMENT 'The zip code of the trip pickup location.',\n",
    "  dropoff_zip INT COMMENT 'The zip code of the trip dropoff location.',\n",
    "  pickup_latitude DOUBLE COMMENT 'The latitude of the trip pickup location.',\n",
    "  pickup_longitude DOUBLE COMMENT 'The longitude of the trip pickup location.',\n",
    "  dropoff_latitude DOUBLE COMMENT 'The latitude of the trip dropoff location.',\n",
    "  dropoff_longitude DOUBLE COMMENT 'The longitude of the trip dropoff location.',\n",
    "  trip_type STRING COMMENT 'Type of trip (e.g., Taxi, Ride Sharing).'\n",
    ")\n",
    "COMMENT 'Contains car trips. It includes details such as the trip duration, pick-up and drop-off locations, and latitude/longitude coordinates. This data can be used to analyze taxi usage patterns, identify high-traffic areas, and track the movement of taxis throughout the city. It can also be used to optimize taxi routes and improve overall taxi service efficiency.'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog_name}.{schema_name}.trips_route_gold (\n",
    "  trip_id BIGINT COMMENT 'Reference to the trip id in trips_silver.',\n",
    "  path_latitude DOUBLE COMMENT 'Latitude of the trip route, allowing tracking of the trip path.',\n",
    "  path_longitude DOUBLE COMMENT 'Longitude of the trip route, allowing tracking of the trip path.',\n",
    "  order LONG COMMENT 'Unique identifier for the trip order, allowing easy reference and tracking.',\n",
    "  tpep_pickup_datetime TIMESTAMP COMMENT 'The timestamp of the taxi trip pickup.',\n",
    "  tpep_dropoff_datetime TIMESTAMP COMMENT 'The timestamp of the taxi trip dropoff.'\n",
    ")\n",
    "COMMENT 'Contains information about trips with their route. It includes latitude and longitude of the path taken. This data can be used to analyze taxi/ride sharing routes, identify congested areas, and optimize taxi routes for improved efficiency and reduced travel times. Additionally, it can help in understanding taxi demand patterns and identifying high-demand areas for taxi services.'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Telematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog_name}.{schema_name}.telematics_bronze (\n",
    "vehicle_id BIGINT COMMENT 'Unique identifier for the vehicle',\n",
    "vin_number STRING COMMENT 'Vehicle Identification Number (VIN)',\n",
    "timestamp TIMESTAMP COMMENT 'Represents the time of data capture',\n",
    "-- Location Data\n",
    "latitude DOUBLE COMMENT 'Latitude coordinate of the vehicle at the time of data capture',\n",
    "longitude DOUBLE COMMENT 'Longitude coordinate of the vehicle at the time of data capture',\n",
    "-- Safety & Diagnostics\n",
    "abs BOOLEAN COMMENT 'Indicates if the Anti-lock Braking System (ABS) was activated.',\n",
    "tpms BOOLEAN COMMENT 'Indicates if the tire pressure monitoring system (TPMS) detected low tire pressure.',\n",
    "brake_pad BOOLEAN COMMENT 'Indicates if the brake pad needs to be changed.',\n",
    "accelerometer BOOLEAN COMMENT 'Indicates if the vehicles acceleration level was high.',\n",
    "blind_spot BOOLEAN COMMENT 'Indicates the presence of a vehicle in the drivers blind spot.',\n",
    "lane_departure BOOLEAN COMMENT 'Indicates if the vehicle departed from its lane.',\n",
    "fcw BOOLEAN COMMENT 'Indicates a potential forward collision detected by the FCW system.',\n",
    "parking_sensor BOOLEAN COMMENT 'Indicates presence of obstacles in the vehicles parking path.',\n",
    "rear_view_camera BOOLEAN COMMENT 'Indicates presence of obstacles detected by the rear-view camera.',\n",
    "engine_light BOOLEAN COMMENT 'Indicates if the engine light is on.',\n",
    "rain_sensor BOOLEAN COMMENT 'Indicates presence of rain detected by the rain sensor.',\n",
    "wiper_blades BOOLEAN COMMENT 'Indicates if the wipers were on.',\n",
    "fog_light BOOLEAN COMMENT 'Indicates if fog lights were activated.',\n",
    "seatbelt_off BOOLEAN COMMENT 'Indicates if a seatbelt was unfastened during the trip.',\n",
    "door_ajar BOOLEAN COMMENT 'Indicates if any of the vehicle doors were open.',\n",
    "-- Fuel & Engine Metrics\n",
    "total_fuel_consumption DOUBLE COMMENT 'Total fuel consumption in liters',\n",
    "fuel_level_percent DOUBLE COMMENT 'Fuel level as a percentage',\n",
    "fuel_level_liters DOUBLE COMMENT 'Fuel level in liters',\n",
    "vehicle_mileage_dashboard DOUBLE COMMENT 'Mileage reading from vehicle dashboard (in kilometers)',\n",
    "vehicle_mileage_computed DOUBLE COMMENT 'Mileage calculated since adapter installation (in kilometers)',\n",
    "engine_speed_rpm INT COMMENT 'Engine speed in RPM',\n",
    "oil_level_status BOOLEAN COMMENT 'Indicates oil level/pressure status',\n",
    "engine_temperature DOUBLE COMMENT 'Engine temperature in Celsius',\n",
    "vehicle_speed DOUBLE COMMENT 'Vehicle speed in km/h',\n",
    "accelerator_pedal_position DOUBLE COMMENT 'Accelerator pedal position as a percentage',\n",
    "-- Lights\n",
    "lights_parking BOOLEAN COMMENT 'Parking lights status',\n",
    "lights_dipped BOOLEAN COMMENT 'Dipped beam headlights status',\n",
    "lights_full_beam BOOLEAN COMMENT 'Full beam headlights status',\n",
    "lights_fog_front BOOLEAN COMMENT 'Front fog lights status',\n",
    "lights_fog_rear BOOLEAN COMMENT 'Rear fog lights status',\n",
    "-- Door & Covers\n",
    "door_front_left BOOLEAN COMMENT 'Front left door open status',\n",
    "door_front_right BOOLEAN COMMENT 'Front right door open status',\n",
    "door_rear_right BOOLEAN COMMENT 'Rear right door open status',\n",
    "door_rear_left BOOLEAN COMMENT 'Rear left door open status',\n",
    "trunk_cover BOOLEAN COMMENT 'Trunk cover open status',\n",
    "engine_hood BOOLEAN COMMENT 'Engine cover (hood) open status',\n",
    "-- Electric Vehicle (EV) Metrics\n",
    "charging_cable_status BOOLEAN COMMENT 'Indicates whether the charging cable is connected',\n",
    "charging_status BOOLEAN COMMENT 'Indicates whether the vehicle is currently charging',\n",
    "battery_level DOUBLE COMMENT 'Battery level as a percentage',\n",
    "vehicle_range_km DOUBLE COMMENT 'Estimated remaining driving range on battery (in kilometers)',\n",
    "battery_temperature DOUBLE COMMENT 'Battery temperature in Celsius',\n",
    "-- CNG & Emissions\n",
    "cng_level DOUBLE COMMENT 'CNG level in percentage or volume',\n",
    "total_cng_consumption DOUBLE COMMENT 'Total compressed natural gas consumed in liters',\n",
    "engine_on_cng BOOLEAN COMMENT 'Indicates if engine is currently running on CNG',\n",
    "adblue_level_dashboard DOUBLE COMMENT 'AdBlue level from dashboard',\n",
    "-- Other Systems\n",
    "seatbelt_driver BOOLEAN COMMENT 'Drivers seat belt status',\n",
    "seatbelt_passenger BOOLEAN COMMENT 'Passengers seat belt status',\n",
    "-- Warning Indicators\n",
    "check_engine BOOLEAN COMMENT 'Check engine warning status',\n",
    "lights_failure BOOLEAN COMMENT 'Light failure warning status',\n",
    "low_tire_pressure BOOLEAN COMMENT 'Low tire pressure warning status'\n",
    "\n",
    ")\n",
    "COMMENT 'This table stores raw telematics data collected from vehicles, including location, fuel and engine metrics, electric vehicle parameters, CNG usage, system diagnostics, and safety indicators. It provides a comprehensive view of vehicle behavior and condition over time, supporting downstream analytics such as predictive maintenance, driver behavior analysis, and geospatial insights.'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contributing Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog_name}.{schema_name}.contributing_factor_tags_lkp_gold (\n",
    "  contributing_factor_grouping STRING COMMENT 'Represents the category or grouping of contributing factors.',\n",
    "  contributing_factor STRING COMMENT 'Identifies the specific contributing factor within the grouping.',\n",
    "  abs BOOLEAN COMMENT 'Represents the Anti-lock Braking System status, indicating if the ABS was activated',\n",
    "  tpms BOOLEAN COMMENT 'Represents the tire pressure monitoring system (TPMS) status, indicating the tire pressure was low',\n",
    "  brake_pad BOOLEAN COMMENT 'Represents the brake pad condition, indicating if brake pad need to be changed',\n",
    "  accelerometer BOOLEAN COMMENT 'Represents the accelerometer data, indicating the vehicles acceleration level was high',\n",
    "  blind_spot BOOLEAN COMMENT 'Represents the blind spot sensor data, indicating the presence or absence of vehicles in the driver blind spot.',\n",
    "  lane_departure BOOLEAN COMMENT 'Represents the lane departure warning system (LDWS) data, indicating the presence or absence of the vehicleposition in its lane.',\n",
    "  fcw BOOLEAN COMMENT 'Represents the forward collision warning (FCW) system data, indicating the presence or absence of potential collisions.',\n",
    "  parking_sensor BOOLEAN COMMENT 'Represents the parking sensor data, indicating the presence or absence of obstacles in the vehicle parking path.',\n",
    "  rear_view_camera BOOLEAN COMMENT 'Represents the rear-view camera data, indicating the presence or absence of obstacles behind the vehicle.',\n",
    "  engine_light BOOLEAN COMMENT 'Represents the engine light status, indicating the presence or absence of engine issues.',\n",
    "  rain_sensor BOOLEAN COMMENT 'Represents the rain sensor data, indicating the presence or absence of rain.',\n",
    "  wiper_baldes BOOLEAN COMMENT 'Represents the wiper status, indicating the wiper was on or off',\n",
    "  fog_light BOOLEAN COMMENT 'Represents the fog light status, indicating the presence or absence of fog.',\n",
    "  seatbelt_off BOOLEAN COMMENT 'Represents the seatbelt status, indicating the presence or absence of seatbelt usage.',\n",
    "  door_ajar BOOLEAN COMMENT 'Represents the door status, indicating the presence or absence of open doors.'\n",
    ")\n",
    "COMMENT 'Contains data on various contributing factors that can impact the performance of a vehicle. It includes information on factors like tire pressure, brake pads, and sensor data. This data can be used to identify potential issues that may affect the safety and reliability of the vehicle, enabling proactive maintenance and repair. Additionally, it can help in tracking the performance of the vehicle over time and identifying trends in the data.'\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog_name}.{schema_name}.weather_history_bronze (\n",
    "  datetime TIMESTAMP COMMENT 'Represents the time when the weather data was recorded.',\n",
    "  date DATE COMMENT 'Represents the date when the weather data was recorded.',\n",
    "  latitude DOUBLE COMMENT 'Represents the latitude of the location where the weather data was recorded.',\n",
    "  longitude DOUBLE COMMENT 'Represents the longitude of the location where the weather data was recorded.',\n",
    "  temperature_2m FLOAT COMMENT 'Represents the temperature at 2 meters above the ground level.',\n",
    "  precipitation FLOAT COMMENT 'Represents the precipitation level, measured in millimeters.',\n",
    "  rain FLOAT COMMENT 'Represents the rainfall level, measured in millimeters.',\n",
    "  snowfall FLOAT COMMENT 'Represents the snowfall level, measured in millimeters.',\n",
    "  snow_depth FLOAT COMMENT 'Represents the snow depth at the location, measured in centimeters.',\n",
    "  weather_code FLOAT COMMENT 'Represents the weather code, which can be used to identify the weather type.',\n",
    "  wind_speed_10m FLOAT COMMENT 'Represents the wind speed at 10 meters above the ground level.',\n",
    "  wind_direction_10m FLOAT COMMENT 'Represents the wind direction at 10 meters above the ground level.',\n",
    "  wind_gusts_10m FLOAT COMMENT 'Represents the wind gusts at 10 meters above the ground level.'\n",
    ")\n",
    "COMMENT 'The table contains historical weather. It includes information such as temperature, precipitation, wind speed, and wind direction. This data can be useful for various purposes, including weather forecasting, historical weather analysis, and climate modeling. For instance, it can help in understanding weather patterns, predicting future weather trends, and assessing the impact of climate change.'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Gold Table - Trip Analytics Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog_name}.{schema_name}.trip_analyics_synthesis_gold (\n",
    "  trip_trip_id LONG COMMENT 'Reference to the trip id in trips_silver.',\n",
    "  trip_start_datetime TIMESTAMP COMMENT 'The timestamp of the taxi trip pickup.',\n",
    "  trip_end_datetime TIMESTAMP COMMENT 'The timestamp of the taxi trip dropoff.',\n",
    "  trip_latitude DOUBLE COMMENT 'Latitude of the trip route, allowing tracking of the trip path.',\n",
    "  trip_longitude DOUBLE COMMENT 'Longitude of the trip route, allowing tracking of the trip path.',\n",
    "  trip_path_order LONG COMMENT 'Unique identifier for the trip order, allowing easy reference and tracking.',\n",
    "  trip_path_description STRING,\n",
    "  trip_h3_index LONG,\n",
    "  collision_crash_date DATE,\n",
    "  collision_crash_time TIMESTAMP,\n",
    "  collision_borough STRING COMMENT 'Represents the borough where the collision took place.',\n",
    "  collision_zip_code STRING COMMENT 'Represents the zip code where the collision took place.',\n",
    "  collision_latitude DOUBLE COMMENT 'Represents the latitude of the collision location.',\n",
    "  collision_longitude DOUBLE COMMENT 'Represents the longitude of the collision location.',\n",
    "  collision_on_street_name STRING COMMENT 'Represents the name of the street where the collision took place.',\n",
    "  collision_cross_street_name STRING COMMENT 'Represents the name of the cross street where the collision took place.',\n",
    "  collision_off_street_name STRING COMMENT 'Represents the name of the off-street location where the collision took place.',\n",
    "  collision_number_of_persons_injured INT,\n",
    "  collision_number_of_persons_killed INT COMMENT 'Represents the number of people who were killed in the collision.',\n",
    "  collision_number_of_pedestrians_injured INT COMMENT 'Represents the number of pedestrians who were injured in the collision.',\n",
    "  collision_number_of_pedestrians_killed INT COMMENT 'Represents the number of pedestrians who were killed in the collision.',\n",
    "  collision_number_of_cyclist_injured INT COMMENT 'Represents the number of cyclists who were injured in the collision.',\n",
    "  collision_number_of_cyclist_killed INT,\n",
    "  collision_number_of_motorist_injured INT,\n",
    "  collision_number_of_motorist_killed INT COMMENT 'Represents the number of motorists who were killed in the collision.',\n",
    "  collision_contributing_factor_vehicle_1 STRING COMMENT 'Represents the first contributing factor of the collision, related to one of the vehicles involved.',\n",
    "  collision_contributing_factor_vehicle_2 STRING COMMENT 'Represents the second contributing factor of the collision, related to one of the vehicles involved.',\n",
    "  collision_contributing_factor_vehicle_3 STRING COMMENT 'Represents the third contributing factor of the collision, if applicable.',\n",
    "  collision_contributing_factor_vehicle_4 STRING COMMENT 'Represents the fourth contributing factor of the collision, if applicable.',\n",
    "  collision_contributing_factor_vehicle_5 STRING COMMENT 'Represents the fifth contributing factor of the collision, if applicable.',\n",
    "  collision_collision_id INT COMMENT 'Unique identifier for the collision, allowing easy reference and tracking.',\n",
    "  collision_vehicle_type_code_1 STRING COMMENT 'First vehicle type code associated with the collision.',\n",
    "  collision_vehicle_type_code_2 STRING COMMENT 'Second vehicle type code associated with the collision.',\n",
    "  collision_vehicle_type_code_3 STRING COMMENT 'Third vehicle type code associated with the collision.',\n",
    "  collision_vehicle_type_code_4 STRING COMMENT 'Fourth vehicle type code associated with the collision.',\n",
    "  collision_vehicle_type_code_5 STRING COMMENT 'Fifth vehicle type code associated with the collision.',\n",
    "  contributing_factor STRING,\n",
    "  weather_temperature_2m FLOAT COMMENT 'Represents the temperature at 2 meters above the ground level.',\n",
    "  weather_precipitation FLOAT COMMENT 'Represents the precipitation level, measured in millimeters.',\n",
    "  weather_rain FLOAT COMMENT 'Represents the rainfall level, measured in millimeters.',\n",
    "  weather_snowfall FLOAT COMMENT 'Represents the snowfall level, measured in millimeters.',\n",
    "  weather_snow_depth FLOAT COMMENT 'Represents the snow depth at the location, measured in centimeters.',\n",
    "  weather_weather_code FLOAT COMMENT 'Represents the weather code, which can be used to identify the weather type.',\n",
    "  weather_wind_speed_10m FLOAT COMMENT 'Represents the wind speed at 10 meters above the ground level.',\n",
    "  weather_wind_direction_10m FLOAT COMMENT 'Represents the wind direction at 10 meters above the ground level.',\n",
    "  weather_wind_gusts_10m FLOAT COMMENT 'Represents the wind gusts at 10 meters above the ground level.',\n",
    "  traffic_volume INT COMMENT 'The total sum of counts collected within a 15-minute increment.',\n",
    "  road_severity STRING COMMENT 'Severity level of the road condition or event.',\n",
    "  road_roadway_name STRING COMMENT 'Name of the affected roadway.',\n",
    "  road_direction_of_travel STRING COMMENT 'Direction of travel on the affected roadway.',\n",
    "  road_description STRING COMMENT 'Description of the road event or condition.',\n",
    "  road_lanes_affected STRING COMMENT 'Information about lanes impacted by the event.',\n",
    "  road_lanes_status STRING COMMENT 'Current status of affected lanes (e.g., closed, open, restricted).',\n",
    "  road_navteq_link_id STRING COMMENT 'Navteq link ID used for mapping the roadway segment.',\n",
    "  road_primary_location STRING COMMENT 'Primary location detail for the road event.',\n",
    "  road_secondary_location STRING COMMENT 'Secondary location reference related to the event.',\n",
    "  road_first_article_city STRING COMMENT 'Primary city associated with the road event.',\n",
    "  road_second_city STRING COMMENT 'Secondary city or neighboring city reference.',\n",
    "  road_event_type STRING COMMENT 'General type of road event (e.g., construction, incident, closure).',\n",
    "  road_event_sub_type STRING COMMENT 'More specific classification of the road event.',\n",
    "  road_reported_date TIMESTAMP COMMENT 'Timestamp when the event was first reported.',\n",
    "  road_start_date TIMESTAMP COMMENT 'Planned or actual start date of the event.',\n",
    "  road_planned_end_date TIMESTAMP COMMENT 'Planned end date of the road event.',\n",
    "  telematics_abs INT,\n",
    "  telematics_accelerometer INT,\n",
    "  telematics_blind_spot INT,\n",
    "  telematics_brake_pad INT,\n",
    "  telematics_door_ajar INT,\n",
    "  telematics_engine_light INT,\n",
    "  telematics_fcw INT,\n",
    "  telematics_fog_light INT,\n",
    "  telematics_lane_departure INT,\n",
    "  telematics_parking_sensor INT,\n",
    "  telematics_rain_sensor INT,\n",
    "  telematics_rear_view_camera INT,\n",
    "  telematics_seatbelt_off INT,\n",
    "  telematics_tpms INT,\n",
    "  telematics_wiper_blades INT,\n",
    "  vehicle_id LONG,\n",
    "  is_collision BOOLEAN\n",
    ")\n",
    "COMMENT 'Contains a summary of trip analytics, including vehicle information, trip details, weather conditions, and contributing factors to collisions. This data can be used for traffic analysis, safety assessments, and operational decision-making. It provides insights into the relationship between weather conditions and traffic incidents, helping to improve road safety and inform transportation planning.'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Implement Medallion Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bronze Tables\n",
    "\n",
    "##### Load files with Auto Loader\n",
    "Simply load from bronze tables using Auto Loader\n",
    "\n",
    "Note: We process compressed files directly, but for large-scale deployments, it’s recommended to decompress files before using AutoLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "volume_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}\"\n",
    "\n",
    "df_casted = (\n",
    "  spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"cloudFiles.validateOptions\", \"false\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{volume_path}/schema/collision_bronze\")\n",
    "    .load(f\"{volume_path}/data/collisions\")\n",
    "        .selectExpr(\n",
    "        \"`CRASH DATE` AS crash_date\",\n",
    "        \"`CRASH TIME` AS crash_time\",\n",
    "        \"`BOROUGH` AS borough\",\n",
    "        \"`ZIP CODE` AS zip_code\",\n",
    "        \"CAST(`LATITUDE` AS DOUBLE) AS latitude\",\n",
    "        \"CAST(`LONGITUDE` AS DOUBLE) AS longitude\",\n",
    "        \"`LOCATION` AS location\",\n",
    "        \"`ON STREET NAME` AS on_street_name\",\n",
    "        \"`CROSS STREET NAME` AS cross_street_name\",\n",
    "        \"`OFF STREET NAME` AS off_street_name\",\n",
    "        \"`NUMBER OF PERSONS INJURED` AS number_of_persons_injured\",\n",
    "        \"CAST(`NUMBER OF PERSONS KILLED` AS INT) AS number_of_persons_killed\",\n",
    "        \"CAST(`NUMBER OF PEDESTRIANS INJURED` AS INT) AS number_of_pedestrians_injured\",\n",
    "        \"CAST(`NUMBER OF PEDESTRIANS KILLED` AS INT) AS number_of_pedestrians_killed\",\n",
    "        \"CAST(`NUMBER OF CYCLIST INJURED` AS INT) AS number_of_cyclist_injured\",\n",
    "        \"`NUMBER OF CYCLIST KILLED` AS number_of_cyclist_killed\",\n",
    "        \"`NUMBER OF MOTORIST INJURED` AS number_of_motorist_injured\",\n",
    "        \"CAST(`NUMBER OF MOTORIST KILLED` AS INT) AS number_of_motorist_killed\",\n",
    "        \"`CONTRIBUTING FACTOR VEHICLE 1` AS contributing_factor_vehicle_1\",\n",
    "        \"`CONTRIBUTING FACTOR VEHICLE 2` AS contributing_factor_vehicle_2\",\n",
    "        \"`CONTRIBUTING FACTOR VEHICLE 3` AS contributing_factor_vehicle_3\",\n",
    "        \"`CONTRIBUTING FACTOR VEHICLE 4` AS contributing_factor_vehicle_4\",\n",
    "        \"`CONTRIBUTING FACTOR VEHICLE 5` AS contributing_factor_vehicle_5\",\n",
    "        \"CAST(`COLLISION_ID` AS INT) AS collision_id\",\n",
    "        \"`VEHICLE TYPE CODE 1` AS vehicle_type_code_1\",\n",
    "        \"`VEHICLE TYPE CODE 2` AS vehicle_type_code_2\",\n",
    "        \"`VEHICLE TYPE CODE 3` AS vehicle_type_code_3\",\n",
    "        \"`VEHICLE TYPE CODE 4` AS vehicle_type_code_4\",\n",
    "        \"`VEHICLE TYPE CODE 5` AS vehicle_type_code_5\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df_casted.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", f\"{volume_path}/checkpoints/collisions_bronze{checkpoint_suffix}\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .toTable(f\"{catalog_name}.{schema_name}.collision_bronze\").awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.table(f\"{catalog_name}.{schema_name}.collision_bronze\").summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "volume_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}\"\n",
    "\n",
    "df_casted = (\n",
    "  spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{volume_path}/schema/road_condition_bronze\")\n",
    "    .load(f\"{volume_path}/data/road_condition\")\n",
    "    .selectExpr(\n",
    "        \"CAST(latitude AS DOUBLE) AS latitude\",\n",
    "        \"CAST(longitude AS DOUBLE) AS longitude\",\n",
    "        \"CAST(ID AS STRING) AS id\",\n",
    "        \"CAST(RegionName AS STRING) AS region_name\",\n",
    "        \"CAST(CountyName AS STRING) AS county_name\",\n",
    "        \"CAST(Severity AS STRING) AS severity\",\n",
    "        \"CAST(RoadwayName AS STRING) AS roadway_name\",\n",
    "        \"CAST(DirectionOfTravel AS STRING) AS direction_of_travel\",\n",
    "        \"CAST(Description AS STRING) AS description\",\n",
    "        \"CAST(LanesAffected AS STRING) AS lanes_affected\",\n",
    "        \"CAST(LanesStatus AS STRING) AS lanes_status\",\n",
    "        \"CAST(NavteqLinkId AS STRING) AS navteq_link_id\",\n",
    "        \"CAST(PrimaryLocation AS STRING) AS primary_location\",\n",
    "        \"CAST(SecondaryLocation AS STRING) AS secondary_location\",\n",
    "        \"CAST(FirstArticleCity AS STRING) AS first_article_city\",\n",
    "        \"CAST(SecondCity AS STRING) AS second_city\",\n",
    "        \"CAST(EventType AS STRING) AS event_type\",\n",
    "        \"CAST(EventSubType AS STRING) AS event_sub_type\",\n",
    "        \"CAST(Reported_Date AS TIMESTAMP) AS reported_date\",\n",
    "        \"CAST(Start_Date AS TIMESTAMP) AS start_date\",\n",
    "        \"CAST(Planned_End_Date AS TIMESTAMP) AS planned_end_date\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df_casted.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", f\"{volume_path}/checkpoints/road_condition_bronze{checkpoint_suffix}\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .toTable(f\"{catalog_name}.{schema_name}.road_condition_bronze\").awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.table(f\"{catalog_name}.{schema_name}.road_condition_bronze\").summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "volume_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}\"\n",
    "\n",
    "df_casted = (\n",
    "  spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"false\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{volume_path}/schema/traffic_volume_counts_bronze\")\n",
    "    .load(f\"{volume_path}/data/traffic_volume\")\n",
    "    .selectExpr(\n",
    "        \"CAST(`RequestID` AS BIGINT) AS requestid\",\n",
    "        \"`Boro` AS boro\",\n",
    "        \"CAST(`Yr` AS INT) AS yr\",\n",
    "        \"CAST(`M` AS INT) AS m\",\n",
    "        \"CAST(`D` AS INT) AS d\",\n",
    "        \"CAST(`HH` AS INT) AS hh\",\n",
    "        \"CAST(`MM` AS INT) AS mm\",\n",
    "        \"CAST(`Vol` AS INT) AS vol\",\n",
    "        \"CAST(`SegmentID` AS BIGINT) AS segmentid\",\n",
    "        \"`WktGeom` AS wktgeom\",\n",
    "        \"`street` AS street\",\n",
    "        \"`fromSt` AS fromst\",\n",
    "        \"`toSt` AS tost\",\n",
    "        \"`Direction` AS direction\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df_casted.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", f\"{volume_path}/checkpoints/traffic_volume_counts_bronze{checkpoint_suffix}\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .toTable(f\"{catalog_name}.{schema_name}.traffic_volume_counts_bronze\").awaitTermination()\n",
    "\n",
    "display(spark.read.table(f\"{catalog_name}.{schema_name}.traffic_volume_counts_bronze\").summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "volume_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}\"\n",
    "\n",
    "df_casted = (\n",
    "  spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"false\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{volume_path}/schema/contributing_factor\")\n",
    "    .load(f\"{volume_path}/data/contributing_factor\")\n",
    "    .selectExpr(\n",
    "        \"`contributing_factor_grouping` AS contributing_factor_grouping\",\n",
    "        \"`contributing_factor` AS contributing_factor\",\n",
    "        \"CAST(`abs` AS BOOLEAN) AS abs\",\n",
    "        \"CAST(`tpms` AS BOOLEAN) AS tpms\",\n",
    "        \"CAST(`brake_pad` AS BOOLEAN) AS brake_pad\",\n",
    "        \"CAST(`accelerometer` AS BOOLEAN) AS accelerometer\",\n",
    "        \"CAST(`blind_spot` AS BOOLEAN) AS blind_spot\",\n",
    "        \"CAST(`lane_departure` AS BOOLEAN) AS lane_departure\",\n",
    "        \"CAST(`fcw` AS BOOLEAN) AS fcw\",\n",
    "        \"CAST(`parking_sensor` AS BOOLEAN) AS parking_sensor\",\n",
    "        \"CAST(`rear_view_camera` AS BOOLEAN) AS rear_view_camera\",\n",
    "        \"CAST(`engine_light` AS BOOLEAN) AS engine_light\",\n",
    "        \"CAST(`rain_sensor` AS BOOLEAN) AS rain_sensor\",\n",
    "        \"CAST(`wiper_baldes` AS BOOLEAN) AS wiper_baldes\",\n",
    "        \"CAST(`fog_light` AS BOOLEAN) AS fog_light\",\n",
    "        \"CAST(`seatbelt_off` AS BOOLEAN) AS seatbelt_off\",\n",
    "        \"CAST(`door_ajar` AS BOOLEAN) AS door_ajar\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df_casted.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", f\"{volume_path}/checkpoints/contributing_factor{checkpoint_suffix}\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .toTable(f\"{catalog_name}.{schema_name}.contributing_factor_tags_lkp_gold\").awaitTermination()\n",
    "\n",
    "display(spark.read.table(f\"{catalog_name}.{schema_name}.contributing_factor_tags_lkp_gold\").summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "volume_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}\"\n",
    "\n",
    "df_casted = (\n",
    "  spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{volume_path}/schema/weather_history_bronze\")\n",
    "    .load(f\"{volume_path}/data/weather\")\n",
    "    .selectExpr(\n",
    "        \"CAST(datetime AS TIMESTAMP) AS datetime\",\n",
    "        \"CAST(date AS DATE) AS date\",\n",
    "        \"CAST(latitude AS DOUBLE) AS latitude\",\n",
    "        \"CAST(longitude AS DOUBLE) AS longitude\",\n",
    "        \"CAST(temperature_2m AS FLOAT) AS temperature_2m\",\n",
    "        \"CAST(precipitation AS FLOAT) AS precipitation\",\n",
    "        \"CAST(rain AS FLOAT) AS rain\",\n",
    "        \"CAST(snowfall AS FLOAT) AS snowfall\",\n",
    "        \"CAST(snow_depth AS FLOAT) AS snow_depth\",\n",
    "        \"CAST(weather_code AS FLOAT) AS weather_code\",\n",
    "        \"CAST(wind_speed_10m AS FLOAT) AS wind_speed_10m\",\n",
    "        \"CAST(wind_direction_10m AS FLOAT) AS wind_direction_10m\",\n",
    "        \"CAST(wind_gusts_10m AS FLOAT) AS wind_gusts_10m\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df_casted.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", f\"{volume_path}/checkpoints/weather_history_bronze{checkpoint_suffix}\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .toTable(f\"{catalog_name}.{schema_name}.weather_history_bronze\").awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "display(spark.read.table(f\"{catalog_name}.{schema_name}.weather_history_bronze\").summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Load Rides from samples catalog**\n",
    "\n",
    "Populates directly from Databricks sample catalog (NYC Taxi Rides Sample). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "  MERGE INTO {catalog_name}.{schema_name}.trips_bronze AS target\n",
    "  USING (\n",
    "    SELECT \n",
    "      MAKE_TIMESTAMP(2024, MONTH(tpep_pickup_datetime), DAY(tpep_pickup_datetime), HOUR(tpep_pickup_datetime), MINUTE(tpep_pickup_datetime), SECOND(tpep_pickup_datetime)) AS tpep_pickup_datetime,\n",
    "      MAKE_TIMESTAMP(2024, MONTH(tpep_dropoff_datetime), DAY(tpep_dropoff_datetime), HOUR(tpep_dropoff_datetime), MINUTE(tpep_dropoff_datetime), SECOND(tpep_dropoff_datetime)) AS tpep_dropoff_datetime,\n",
    "      trip_distance,\n",
    "      NULL AS fare_amount,\n",
    "      pickup_zip,\n",
    "      dropoff_zip,\n",
    "      'Taxi' AS trip_type\n",
    "    FROM samples.nyctaxi.trips\n",
    "  ) AS source\n",
    "  ON FALSE\n",
    "  WHEN NOT MATCHED THEN INSERT (\n",
    "    tpep_pickup_datetime,\n",
    "    tpep_dropoff_datetime,\n",
    "    trip_distance,\n",
    "    fare_amount,\n",
    "    pickup_zip,\n",
    "    dropoff_zip,\n",
    "    trip_type\n",
    "  )\n",
    "  VALUES (\n",
    "    source.tpep_pickup_datetime,\n",
    "    source.tpep_dropoff_datetime,\n",
    "    source.trip_distance,\n",
    "    source.fare_amount,\n",
    "    source.pickup_zip,\n",
    "    source.dropoff_zip,\n",
    "    source.trip_type\n",
    "  );\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build synthetic Telematics data\n",
    "\n",
    "Generate 1M synthetic records using  [Databricks Labs Data Generator (dbldatagen)](https://github.com/databrickslabs/dbldatagen)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "from dbldatagen import DataGenerator\n",
    "from pyspark.sql.functions import col, when, rand\n",
    "\n",
    "data_gen = (DataGenerator(spark, rows=1000000, partitions=8)\n",
    "    .withColumn(\"vehicle_id\", \"bigint\", minValue=1, maxValue=1000000)\n",
    "    .withColumn(\"vin_number\", \"string\", format=\"vin\")\n",
    "    .withColumn(\"timestamp\", \"timestamp\", begin=\"2024-01-01 00:00:00\", end=\"2024-12-31 23:59:59\", interval=\"1 second\")\n",
    "    .withColumn(\"latitude\", \"double\", minValue=40.477399, maxValue=45.01585)\n",
    "    .withColumn(\"longitude\", \"double\", minValue=-79.76259, maxValue=-71.7517)\n",
    "    \n",
    "    .withColumn(\"abs\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"tpms\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"brake_pad\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"accelerometer\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"blind_spot\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"lane_departure\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"fcw\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"parking_sensor\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"rear_view_camera\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"engine_light\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"rain_sensor\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"wiper_blades\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"fog_light\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"seatbelt_off\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"door_ajar\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    \n",
    "    .withColumn(\"total_fuel_consumption\", \"double\", minValue=0, maxValue=1000)\n",
    "    .withColumn(\"fuel_level_percent\", \"double\", minValue=0, maxValue=100)\n",
    "    .withColumn(\"fuel_level_liters\", \"double\", minValue=0, maxValue=50)\n",
    "    .withColumn(\"vehicle_mileage_dashboard\", \"double\", minValue=0, maxValue=300000)\n",
    "    .withColumn(\"vehicle_mileage_computed\", \"double\", minValue=0, maxValue=300000)\n",
    "    .withColumn(\"engine_speed_rpm\", \"integer\", minValue=0, maxValue=8000)\n",
    "    .withColumn(\"oil_level_status\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"engine_temperature\", \"double\", minValue=-40, maxValue=150)\n",
    "    .withColumn(\"vehicle_speed\", \"double\", minValue=0, maxValue=200)\n",
    "    .withColumn(\"accelerator_pedal_position\", \"double\", minValue=0, maxValue=100)\n",
    "\n",
    "    .withColumn(\"lights_parking\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"lights_dipped\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"lights_full_beam\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"lights_fog_front\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"lights_fog_rear\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "\n",
    "    .withColumn(\"door_front_left\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"door_front_right\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"door_rear_right\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"door_rear_left\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"trunk_cover\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"engine_hood\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "\n",
    "    .withColumn(\"charging_cable_status\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"charging_status\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"battery_level\", \"double\", minValue=0, maxValue=100)\n",
    "    .withColumn(\"vehicle_range_km\", \"double\", minValue=0, maxValue=500)\n",
    "    .withColumn(\"battery_temperature\", \"double\", minValue=-40, maxValue=60)\n",
    "    \n",
    "    .withColumn(\"cng_level\", \"double\", minValue=0, maxValue=100)\n",
    "    .withColumn(\"total_cng_consumption\", \"double\", minValue=0, maxValue=1000)\n",
    "    .withColumn(\"engine_on_cng\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"adblue_level_dashboard\", \"double\", minValue=0, maxValue=100)\n",
    "    \n",
    "    .withColumn(\"seatbelt_driver\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"seatbelt_passenger\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    \n",
    "    .withColumn(\"check_engine\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"lights_failure\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    "    .withColumn(\"low_tire_pressure\", \"boolean\", expr=\"floor(rand() * 2)\")\n",
    ")\n",
    "\n",
    "# Build the DataFrame\n",
    "df_telematics = data_gen.build()\n",
    "\n",
    "# Apply rare condition simulations (1% high acceleration, speed, rpm)\n",
    "df_telematics = df_telematics.withColumn(\n",
    "    \"accelerator_pedal_position\",\n",
    "    when(rand() < 0.01, 100.0).otherwise(col(\"accelerator_pedal_position\"))\n",
    ")\n",
    "\n",
    "df_telematics = df_telematics.withColumn(\n",
    "    \"vehicle_speed\",\n",
    "    when(rand() < 0.01, 200.0).otherwise(col(\"vehicle_speed\"))\n",
    ")\n",
    "\n",
    "df_telematics = df_telematics.withColumn(\n",
    "    \"engine_speed_rpm\",\n",
    "    when(rand() < 0.01, 8000).otherwise(col(\"engine_speed_rpm\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "display(df_telematics.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "# Write the data to the Delta table\n",
    "df_telematics.write.mode(\"append\").insertInto(f\"{catalog_name}.{schema_name}.telematics_bronze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silver Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Collisions \n",
    "\n",
    "Perform clean up and basic transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, col, concat_ws, unix_timestamp, regexp_replace, trim\n",
    "\n",
    "df = spark.sql(f\"select * from {catalog_name}.{schema_name}.collision_bronze\")\n",
    "df = df.drop(\"location\")\n",
    "\n",
    "# Clean date\n",
    "df = df.withColumn(\"crash_date\", regexp_replace(\"crash_date\", \"[a-zA-Z.]\", \"\"))\n",
    "df = df.withColumn(\"crash_date\", to_date(col(\"crash_date\"), \"MM/dd/yyyy\"))\n",
    "\n",
    "# Clean and sanitize crash_time\n",
    "df = df.withColumn(\"crash_time\", trim(col(\"crash_time\")))\n",
    "\n",
    "# Filter out rows where crash_time doesn't look like a time (e.g., missing digits or non-numeric)\n",
    "df = df.filter(col(\"crash_time\").rlike(\"^[0-9]{1,2}:[0-9]{2}$\"))\n",
    "\n",
    "# Safely parse timestamp\n",
    "df = df.withColumn(\n",
    "    \"crash_time\",\n",
    "    unix_timestamp(concat_ws(\" \", col(\"crash_date\"), col(\"crash_time\")), \"yyyy-MM-dd H:mm\").cast(\"timestamp\")\n",
    ")\n",
    "\n",
    "# Cast columns\n",
    "df = df.withColumn(\"number_of_persons_injured\", col(\"number_of_persons_injured\").cast(\"int\"))\n",
    "df = df.withColumn(\"number_of_cyclist_killed\", col(\"number_of_cyclist_killed\").cast(\"int\"))\n",
    "df = df.withColumn(\"number_of_motorist_injured\", col(\"number_of_motorist_injured\").cast(\"int\"))\n",
    "\n",
    "# Final filter\n",
    "df = df.filter(col(\"crash_date\").isNotNull())\n",
    "\n",
    "# Write to table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\n",
    "    f\"{catalog_name}.{schema_name}.collision_silver\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Volume**\n",
    "\n",
    "Build Silver Table for Traffic Volume using Databricks Built-in Geospatial Function\n",
    "\n",
    "For demo purposes, to speed up the pipeline run, we’re populating the silver layer with events from 2024 only.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "WITH computed AS (\n",
    "  SELECT \n",
    "    vol,\n",
    "    ST_Transform(ST_Centroid(ST_GeomFromText(wktgeom, 2263)), 4326) AS geom,\n",
    "    MAKE_TIMESTAMP(yr, m, d, hh, mm, 0) AS timestamp\n",
    "  FROM {catalog_name}.{schema_name}.traffic_volume_counts_bronze\n",
    "  WHERE wktgeom IS NOT NULL AND vol IS NOT NULL and yr = 2024\n",
    ")\n",
    "MERGE INTO {catalog_name}.{schema_name}.traffic_volume_counts_silver AS target\n",
    "USING (\n",
    "  SELECT\n",
    "    vol,\n",
    "    ST_Y(geom) AS lat,\n",
    "    ST_X(geom) AS long,\n",
    "    timestamp,\n",
    "    h3_longlatash3(ST_X(geom), ST_Y(geom), 9) AS h3_index\n",
    "  FROM computed\n",
    ") AS source\n",
    "ON target.h3_index = source.h3_index AND target.timestamp = source.timestamp\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Trips**\n",
    "\n",
    "Compute latitude and longitude from zip_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using UDF in UC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "#spark.sql(f\"\"\"\n",
    "#CREATE OR REPLACE FUNCTION get_lat_long_from_zipcode(zipcode STRING)\n",
    "#RETURNS STRUCT<latitude: DOUBLE, longitude: DOUBLE>\n",
    "#LANGUAGE PYTHON\n",
    "#ENVIRONMENT (\n",
    "#  dependencies = '[\"requests\"]',\n",
    "#  environment_version = \"None\"\n",
    "#)\n",
    "#AS $$\n",
    "#import requests\n",
    "#url = f\"http://api.zippopotam.us/us/{zipcode}\"\n",
    "#response = requests.get(url)\n",
    "#if response.status_code != 200:\n",
    "#    return None\n",
    "#data = response.json()\n",
    "#try:\n",
    "#    latitude = float(data['places'][0]['latitude'])\n",
    "#    longitude = float(data['places'][0]['longitude'])\n",
    "#except (KeyError, ValueError):\n",
    "#    return None\n",
    "#return (latitude, longitude)\n",
    "#$$;\n",
    "#\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "sql_query = f\"\"\"\n",
    "MERGE INTO {catalog_name}.{schema_name}.trips_silver AS target\n",
    "USING (\n",
    "  SELECT\n",
    "    tpep_pickup_datetime,\n",
    "    tpep_dropoff_datetime,\n",
    "    trip_distance,\n",
    "    pickup_zip,\n",
    "    dropoff_zip,\n",
    "    {catalog_name}.{schema_name}.get_lat_long_from_zipcode(pickup_zip).latitude AS pickup_latitude,\n",
    "    {catalog_name}.{schema_name}.get_lat_long_from_zipcode(pickup_zip).longitude AS pickup_longitude,\n",
    "    {catalog_name}.{schema_name}.get_lat_long_from_zipcode(dropoff_zip).latitude AS dropoff_latitude,\n",
    "    {catalog_name}.{schema_name}.get_lat_long_from_zipcode(dropoff_zip).longitude AS dropoff_longitude,\n",
    "    trip_type\n",
    "  FROM {catalog_name}.{schema_name}.trips_bronze\n",
    ") AS source\n",
    "ON target.tpep_pickup_datetime = source.tpep_pickup_datetime\n",
    "   AND target.tpep_dropoff_datetime = source.tpep_dropoff_datetime\n",
    "   AND target.pickup_zip = source.pickup_zip\n",
    "   AND target.dropoff_zip = source.dropoff_zip\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "  target.trip_distance = source.trip_distance,\n",
    "  target.pickup_latitude = source.pickup_latitude,\n",
    "  target.pickup_longitude = source.pickup_longitude,\n",
    "  target.dropoff_latitude = source.dropoff_latitude,\n",
    "  target.dropoff_longitude = source.dropoff_longitude,\n",
    "  target.trip_type = source.trip_type\n",
    "WHEN NOT MATCHED THEN INSERT (\n",
    "  tpep_pickup_datetime,\n",
    "  tpep_dropoff_datetime,\n",
    "  trip_distance,\n",
    "  pickup_zip,\n",
    "  dropoff_zip,\n",
    "  pickup_latitude,\n",
    "  pickup_longitude,\n",
    "  dropoff_latitude,\n",
    "  dropoff_longitude,\n",
    "  trip_type\n",
    ") VALUES (\n",
    "  source.tpep_pickup_datetime,\n",
    "  source.tpep_dropoff_datetime,\n",
    "  source.trip_distance,\n",
    "  source.pickup_zip,\n",
    "  source.dropoff_zip,\n",
    "  source.pickup_latitude,\n",
    "  source.pickup_longitude,\n",
    "  source.dropoff_latitude,\n",
    "  source.dropoff_longitude,\n",
    "  source.trip_type\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "#spark.sql(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute latitude/longitude using traditional UDFs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pgeocode\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "\n",
    "\n",
    "# Define the schema for the output\n",
    "schema = StructType([\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True)\n",
    "])\n",
    "nomi = pgeocode.Nominatim('US')\n",
    "\n",
    "# Define the Pandas UDF\n",
    "@pandas_udf(schema)\n",
    "def get_lat_long_from_zipcode(zipcodes: pd.Series) -> pd.DataFrame:\n",
    "    results = zipcodes.apply(lambda x: nomi.query_postal_code(x).to_dict())\n",
    "    latitudes = results.apply(lambda x: x['latitude'] if not pd.isna(x['latitude']) else None)\n",
    "    longitudes = results.apply(lambda x: x['longitude'] if not pd.isna(x['longitude']) else None)\n",
    "    return pd.DataFrame({'latitude': latitudes, 'longitude': longitudes})\n",
    "\n",
    "trips_df = spark.sql(f\"SELECT * FROM {catalog_name}.{schema_name}.trips_bronze\")\n",
    "\n",
    "# Apply the Pandas UDF to the pickup_zip and dropoff_zip columns\n",
    "trips_df_with_coords = trips_df.withColumn(\n",
    "    \"pickup\", get_lat_long_from_zipcode(trips_df.pickup_zip)\n",
    ").withColumn(\n",
    "    \"dropoff\", get_lat_long_from_zipcode(trips_df.dropoff_zip)\n",
    ")\n",
    "\n",
    "# Select the original columns along with the new latitude and longitude columns\n",
    "result_df = trips_df_with_coords.select(\n",
    "    \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"trip_distance\", \"pickup_zip\", \"dropoff_zip\", trips_df_with_coords[\"pickup.latitude\"].alias(\"pickup_latitude\"), trips_df_with_coords[\"pickup.longitude\"].alias(\"pickup_longitude\") , trips_df_with_coords[\"dropoff.latitude\"].alias(\"dropoff_latitude\"), trips_df_with_coords[\"dropoff.longitude\"].alias(\"dropoff_longitude\"), \"trip_type\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "display(result_df.head(5))\n",
    "result_df.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.trips_silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gold Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Compute Routes for trips\n",
    "\n",
    "We need to compute routes since our trips only have pickup and drop-off. We want to correlate any collision, road volume, and weather along the way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "class RouteProcessor:\n",
    "    def __init__(self):\n",
    "        import osmnx as ox\n",
    "        self.ox = ox\n",
    "        self.nx = __import__('networkx')\n",
    "        self.graph = None\n",
    "\n",
    "    def load_graph_from_location(self, location):\n",
    "        self.graph = self.ox.graph_from_place(location, network_type=\"drive\")\n",
    "\n",
    "    def set_graph(self, graph):\n",
    "        self.graph = graph\n",
    "\n",
    "    def load_graph_from_dataset(self, trips_df):\n",
    "        bounds = trips_df.selectExpr(\n",
    "            \"min(pickup_latitude) as min_lat\",\n",
    "            \"max(pickup_latitude) as max_lat\",\n",
    "            \"min(pickup_longitude) as min_lon\",\n",
    "            \"max(pickup_longitude) as max_lon\",\n",
    "            \"min(dropoff_latitude) as min_dlat\",\n",
    "            \"max(dropoff_latitude) as max_dlat\",\n",
    "            \"min(dropoff_longitude) as min_dlon\",\n",
    "            \"max(dropoff_longitude) as max_dlon\"\n",
    "        ).collect()[0]\n",
    "\n",
    "        min_lat = min(bounds.min_lat, bounds.min_dlat) - 0.01\n",
    "        max_lat = max(bounds.max_lat, bounds.max_dlat) + 0.01\n",
    "        min_lon = min(bounds.min_lon, bounds.min_dlon) - 0.01\n",
    "        max_lon = max(bounds.max_lon, bounds.max_dlon) + 0.01\n",
    "\n",
    "        bbox = (min_lon, min_lat, max_lon, max_lat)\n",
    "        logger.info(f\"Loading graph for bbox (west, south, east, north): {bbox}\")\n",
    "        self.graph = self.ox.graph_from_bbox(bbox=bbox, network_type=\"drive\")\n",
    "\n",
    "    def _get_route_path(self, pickup_lat, pickup_lon, dropoff_lat, dropoff_lon):\n",
    "        try:\n",
    "            orig_node = self.ox.nearest_nodes(self.graph, pickup_lon, pickup_lat)\n",
    "            dest_node = self.ox.nearest_nodes(self.graph, dropoff_lon, dropoff_lat)\n",
    "            route = self.nx.shortest_path(self.graph, orig_node, dest_node, weight='length')\n",
    "            lats, lons = zip(*[(self.graph.nodes[node]['y'], self.graph.nodes[node]['x']) for node in route])\n",
    "            return list(lats), list(lons)\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Routing failed: {e}\")\n",
    "            logger.info(f\"Pickup: ({pickup_lat}, {pickup_lon}) | Dropoff: ({dropoff_lat}, {dropoff_lon})\")\n",
    "            return [], []\n",
    "\n",
    "    def compute_routes_single_node(self, trips_df):\n",
    "        import pandas as pd\n",
    "        import concurrent.futures\n",
    "\n",
    "        pandas_df = trips_df.select(\n",
    "            \"id\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\",\n",
    "            \"pickup_zip\", \"dropoff_zip\", \n",
    "            \"pickup_latitude\", \"pickup_longitude\", \n",
    "            \"dropoff_latitude\", \"dropoff_longitude\"\n",
    "        ).dropna().distinct().toPandas()\n",
    "\n",
    "        def process_row(row):\n",
    "            # Compute the route path (list of latitudes and longitudes)\n",
    "            path_lat, path_lon = self._get_route_path(\n",
    "                row['pickup_latitude'], row['pickup_longitude'],\n",
    "                row['dropoff_latitude'], row['dropoff_longitude']\n",
    "            )\n",
    "            exploded_rows = []\n",
    "            for i, (lat, lon) in enumerate(zip(path_lat, path_lon)):\n",
    "                # *** Change 2: Populate the output with new schema columns ***\n",
    "                exploded_rows.append({\n",
    "                    \"trip_id\": row[\"id\"],\n",
    "                    \"tpep_pickup_datetime\": row[\"tpep_pickup_datetime\"],\n",
    "                    \"tpep_dropoff_datetime\": row[\"tpep_dropoff_datetime\"],\n",
    "                    \"path_latitude\": lat,\n",
    "                    \"path_longitude\": lon,\n",
    "                    \"order\": i\n",
    "                })\n",
    "            return exploded_rows\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            nested_results = list(executor.map(process_row, [row for _, row in pandas_df.iterrows()]))\n",
    "\n",
    "        # Flatten nested list of exploded routes\n",
    "        all_points = [point for sublist in nested_results for point in sublist]\n",
    "        enriched_df = pd.DataFrame(all_points)\n",
    "        \n",
    "        return enriched_df\n",
    "\n",
    "\n",
    "    def compute_routes_multi_node(self,trips_df,logger, graph_path):\n",
    "        import pandas as pd\n",
    "        from pyspark.sql.functions import col\n",
    "        from pyspark.sql.types import (\n",
    "            StructType, StructField, LongType, DoubleType, IntegerType, TimestampType\n",
    "        )\n",
    "\n",
    "        # Define output schema for the result DataFrame\n",
    "        route_schema = StructType([\n",
    "            StructField(\"trip_id\", LongType()),\n",
    "            StructField(\"tpep_pickup_datetime\", TimestampType()),\n",
    "            StructField(\"tpep_dropoff_datetime\", TimestampType()),\n",
    "            StructField(\"path_latitude\", DoubleType()),\n",
    "            StructField(\"path_longitude\", DoubleType()),\n",
    "            StructField(\"order\", IntegerType())\n",
    "        ])\n",
    "\n",
    "        # Load the graph once and broadcast it\n",
    "        import osmnx as ox\n",
    "        import networkx as nx\n",
    "        graph = ox.load_graphml(graph_path)\n",
    "        broadcast_graph = trips_df.sparkSession.sparkContext.broadcast(graph)\n",
    "\n",
    "        logger.info(f\"Broadcasting graph finished.\")\n",
    "\n",
    "        # Define the route processor as a Pandas function\n",
    "        def route_processor(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "            import networkx as nx\n",
    "            # Retrieve the broadcast graph instead of reloading it every time.\n",
    "            graph = broadcast_graph.value\n",
    "\n",
    "            results = []\n",
    "            for _, row in pdf.iterrows():\n",
    "                try:\n",
    "                    orig_node = ox.nearest_nodes(graph, row['pickup_longitude'], row['pickup_latitude'])\n",
    "                    dest_node = ox.nearest_nodes(graph, row['dropoff_longitude'], row['dropoff_latitude'])\n",
    "                    route = nx.shortest_path(graph, orig_node, dest_node, weight='length')\n",
    "                    lats, lons = zip(*[(graph.nodes[n]['y'], graph.nodes[n]['x']) for n in route])\n",
    "                    for i, (lat, lon) in enumerate(zip(lats, lons)):\n",
    "                        results.append({\n",
    "                            \"trip_id\": row[\"id\"],\n",
    "                            \"tpep_pickup_datetime\": row[\"tpep_pickup_datetime\"],\n",
    "                            \"tpep_dropoff_datetime\": row[\"tpep_dropoff_datetime\"],\n",
    "                            \"path_latitude\": lat,\n",
    "                            \"path_longitude\": lon,\n",
    "                            \"order\": i\n",
    "                        })\n",
    "                except Exception as Ex:\n",
    "                    print(f\"Error processing trip {row['id']}: {Ex}\")\n",
    "                    continue\n",
    "            return pd.DataFrame(results)\n",
    "\n",
    "        # Prepare and clean the input DataFrame\n",
    "        selected_df = trips_df.select(\n",
    "            col(\"id\"),\n",
    "            col(\"tpep_pickup_datetime\"),\n",
    "            col(\"tpep_dropoff_datetime\"),\n",
    "            col(\"pickup_latitude\"),\n",
    "            col(\"pickup_longitude\"),\n",
    "            col(\"dropoff_latitude\"),\n",
    "            col(\"dropoff_longitude\")\n",
    "        ).dropna().distinct()\n",
    "\n",
    "        \n",
    "        # Apply the route processor using applyInPandas with groupBy(\"id\")\n",
    "\n",
    "        # Repartition the DataFrame to ensure applyInPandas will autoscale on a small data set (250K rows).\n",
    "        result_df = selected_df.repartition(50000).groupBy(\"id\").applyInPandas(route_processor, schema=route_schema).coalesce(20)\n",
    "\n",
    "        return result_df\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and save graph to volume because all Spark nodes need access to Graph file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "processor = RouteProcessor()\n",
    "processor.load_graph_from_location(\"New York City, New York, USA\") \n",
    "\n",
    "graph_path = None\n",
    "\n",
    "\n",
    "local_file_path = \"./cache/graph.graphml\"\n",
    "volume_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}\"\n",
    "\n",
    "processor.ox.save_graphml(processor.graph, filepath=local_file_path)\n",
    "\n",
    "upload_file_to_volume(local_file_path, volume_path, \"graph.graphml\")\n",
    "\n",
    "graph_path = f\"{volume_path}/graph.graphml\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the route\n",
    "\n",
    "_**Note:**_ Based on the sample dataset, this can take an average of one hour to compute routes for 250K records on a 2-node small cluster. Scale up the cluster to process faster.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "trips_df = spark.table(f\"{catalog_name}.{schema_name}.trips_silver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "logger.info(f\"multi_node_route: {multi_node_route}\")\n",
    "logger.info(f\"is_spark_connect: {is_spark_connect}\")\n",
    "\n",
    "logger.info(f\"Computing route for {trips_df.count()} trips\")\n",
    "\n",
    "trips_df = processor.compute_routes_multi_node(trips_df, logger, graph_path)\n",
    "\n",
    "#our multi node implementation does not work on Spark Connect sessions yet.\n",
    "\n",
    "#if multi_node_route and multi_node_route == True and is_spark_connect == False :\n",
    "\n",
    "#    trips_df = processor.compute_routes_multi_node(trips_df,logger, graph_path)\n",
    "#else:\n",
    "    \n",
    "#    pandas_df = processor.compute_routes_single_node(trips_df)\n",
    "    \n",
    "#    trips_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "logger.info(f\"Generated {trips_df.count()} route points\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "trips_df = trips_df.select(\n",
    "    col(\"trip_id\").cast(\"bigint\"),\n",
    "    col(\"path_latitude\").cast(\"double\"),\n",
    "    col(\"path_longitude\").cast(\"double\"),\n",
    "    col(\"order\").cast(\"long\"),\n",
    "    col(\"tpep_pickup_datetime\"),\n",
    "    col(\"tpep_dropoff_datetime\")\n",
    ")\n",
    "\n",
    "# Write the final DataFrame to the trips_route_gold table\n",
    "trips_df.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.trips_route_gold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot a Route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from keplergl import KeplerGl\n",
    "\n",
    "def get_route_geojson(route_df: pd.DataFrame):\n",
    "    route_df = route_df.sort_values(\"order\")\n",
    "    coordinates = list(zip(route_df['path_longitude'], route_df['path_latitude']))\n",
    "    return {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": [{\n",
    "            \"type\": \"Feature\",\n",
    "            \"geometry\": {\n",
    "                \"type\": \"LineString\",\n",
    "                \"coordinates\": coordinates\n",
    "            },\n",
    "            \"properties\": {}\n",
    "        }]\n",
    "    }\n",
    "\n",
    "single_route_df = spark.table(f\"{catalog_name}.{schema_name}.trips_route_gold\").filter(\"trip_id = 10000\").orderBy(\"order\")\n",
    "pandas_route = single_route_df.toPandas()\n",
    "\n",
    "geojson_data = get_route_geojson(pandas_route)\n",
    "\n",
    "kepler_map = KeplerGl(height=500)\n",
    "kepler_map.add_data(data=geojson_data, name=\"Route Path\")\n",
    "kepler_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate final table - trip analytics \n",
    "\n",
    "Build final table with several features, combining trips, road condition, weather, traffic volume, and telematics.\n",
    "\n",
    "_Note: We use rounding to two decimal places when joining tables on latitude and longitude, which results in approximately 800 meters of precision loss. We’re intentionally doing this, as our analytics is not at street-level granularity._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "# Build trip analytics synthesis gold table\n",
    "sql_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog_name}.{schema_name}.trip_analyics_synthesis_gold\n",
    "CLUSTER BY (trip_h3_index, trip_start_datetime)\n",
    "AS\n",
    "WITH ordered_routes AS (\n",
    "  SELECT\n",
    "    trip_id,\n",
    "    tpep_pickup_datetime,\n",
    "    tpep_dropoff_datetime,\n",
    "    path_latitude AS trip_latitude,\n",
    "    path_longitude AS trip_longitude,\n",
    "    order as path_order,\n",
    "    COUNT(*) OVER (\n",
    "      PARTITION BY trip_id\n",
    "    ) AS total_points\n",
    "  FROM {catalog_name}.{schema_name}.trips_route_gold\n",
    "),\n",
    "route_with_labels AS (\n",
    "  SELECT DISTINCT\n",
    "    trip_id,\n",
    "    tpep_pickup_datetime,\n",
    "    tpep_dropoff_datetime,\n",
    "    trip_latitude,\n",
    "    trip_longitude,\n",
    "    path_order,\n",
    "    CASE\n",
    "      WHEN path_order <= total_points * 0.30 THEN 'init'\n",
    "      WHEN path_order > total_points * 0.30 \n",
    "         AND path_order < total_points * 0.80 THEN 'mid'\n",
    "      ELSE 'end'\n",
    "    END AS path_description,\n",
    "    h3_longlatash3(trip_longitude, trip_latitude, 9) AS trip_h3_index\n",
    "  FROM ordered_routes\n",
    "),\n",
    "trip_with_collision AS (\n",
    "  SELECT\n",
    "    t.trip_id AS trip_trip_id,\n",
    "    t.tpep_pickup_datetime AS trip_start_datetime,\n",
    "    t.tpep_dropoff_datetime AS trip_end_datetime,\n",
    "    t.trip_latitude AS trip_latitude,\n",
    "    t.trip_longitude AS trip_longitude,\n",
    "    t.path_order AS trip_path_order,\n",
    "    t.path_description AS trip_path_description,\n",
    "    t.trip_h3_index AS trip_h3_index,\n",
    "    c.crash_date AS collision_crash_date,\n",
    "    c.crash_time AS collision_crash_time,\n",
    "    c.borough AS collision_borough,\n",
    "    c.zip_code AS collision_zip_code,\n",
    "    c.latitude AS collision_latitude,\n",
    "    c.longitude AS collision_longitude,\n",
    "    c.on_street_name AS collision_on_street_name,\n",
    "    c.cross_street_name AS collision_cross_street_name,\n",
    "    c.off_street_name AS collision_off_street_name,\n",
    "    c.number_of_persons_injured AS collision_number_of_persons_injured,\n",
    "    c.number_of_persons_killed AS collision_number_of_persons_killed,\n",
    "    c.number_of_pedestrians_injured AS collision_number_of_pedestrians_injured,\n",
    "    c.number_of_pedestrians_killed AS collision_number_of_pedestrians_killed,\n",
    "    c.number_of_cyclist_injured AS collision_number_of_cyclist_injured,\n",
    "    c.number_of_cyclist_killed AS collision_number_of_cyclist_killed,\n",
    "    c.number_of_motorist_injured AS collision_number_of_motorist_injured,\n",
    "    c.number_of_motorist_killed AS collision_number_of_motorist_killed,\n",
    "    c.contributing_factor_vehicle_1 AS collision_contributing_factor_vehicle_1,\n",
    "    c.contributing_factor_vehicle_2 AS collision_contributing_factor_vehicle_2,\n",
    "    c.contributing_factor_vehicle_3 AS collision_contributing_factor_vehicle_3,\n",
    "    c.contributing_factor_vehicle_4 AS collision_contributing_factor_vehicle_4,\n",
    "    c.contributing_factor_vehicle_5 AS collision_contributing_factor_vehicle_5,\n",
    "    c.collision_id AS collision_collision_id,\n",
    "    c.vehicle_type_code_1 AS collision_vehicle_type_code_1,\n",
    "    c.vehicle_type_code_2 AS collision_vehicle_type_code_2,\n",
    "    c.vehicle_type_code_3 AS collision_vehicle_type_code_3,\n",
    "    c.vehicle_type_code_4 AS collision_vehicle_type_code_4,\n",
    "    c.vehicle_type_code_5 AS collision_vehicle_type_code_5\n",
    "  FROM route_with_labels t\n",
    "  LEFT JOIN {catalog_name}.{schema_name}.collision_silver c\n",
    "    ON ROUND(t.trip_longitude,2) = ROUND(c.longitude,2)\n",
    "    AND ROUND(t.trip_latitude,2) = ROUND(c.latitude,2)\n",
    "    AND c.crash_time BETWEEN CAST(t.tpep_pickup_datetime AS DATE) AND CAST(t.tpep_dropoff_datetime AS DATE)\n",
    "),\n",
    "trip_with_cf AS (\n",
    "  SELECT\n",
    "    t.*,\n",
    "    CASE\n",
    "      WHEN collision_contributing_factor_vehicle_1 IS NOT NULL AND collision_contributing_factor_vehicle_1 <> 'Unspecified'\n",
    "      THEN collision_contributing_factor_vehicle_1\n",
    "      WHEN collision_contributing_factor_vehicle_2 IS NOT NULL AND collision_contributing_factor_vehicle_2 <> 'Unspecified'\n",
    "      THEN collision_contributing_factor_vehicle_2\n",
    "      WHEN collision_contributing_factor_vehicle_3 IS NOT NULL AND collision_contributing_factor_vehicle_3 <> 'Unspecified'\n",
    "      THEN collision_contributing_factor_vehicle_3\n",
    "      WHEN collision_contributing_factor_vehicle_4 IS NOT NULL AND collision_contributing_factor_vehicle_4 <> 'Unspecified'\n",
    "      THEN collision_contributing_factor_vehicle_4\n",
    "      ELSE collision_contributing_factor_vehicle_5\n",
    "    END AS contributing_factor\n",
    "  FROM trip_with_collision t\n",
    "),\n",
    "trip_with_weather AS (\n",
    "  SELECT\n",
    "    t.*,\n",
    "    w.temperature_2m AS weather_temperature_2m,\n",
    "    w.precipitation AS weather_precipitation,\n",
    "    w.rain AS weather_rain,\n",
    "    w.snowfall AS weather_snowfall,\n",
    "    w.snow_depth AS weather_snow_depth,\n",
    "    w.weather_code AS weather_weather_code,\n",
    "    w.wind_speed_10m AS weather_wind_speed_10m,\n",
    "    w.wind_direction_10m AS weather_wind_direction_10m,\n",
    "    w.wind_gusts_10m AS weather_wind_gusts_10m\n",
    "  FROM trip_with_cf t\n",
    "  LEFT JOIN {catalog_name}.{schema_name}.weather_history_bronze w\n",
    "    ON ROUND(t.trip_longitude,2) = ROUND(w.longitude,2)\n",
    "    AND ROUND(t.trip_latitude,2) = ROUND(w.latitude,2)\n",
    "    AND CAST(w.datetime AS DATE) BETWEEN CAST(t.trip_start_datetime AS DATE) AND CAST(t.trip_end_datetime AS DATE)\n",
    "),\n",
    "trip_with_volume AS (\n",
    "  SELECT\n",
    "    t.*,\n",
    "    v.vol AS traffic_volume\n",
    "  FROM trip_with_weather t\n",
    "  LEFT JOIN {catalog_name}.{schema_name}.traffic_volume_counts_silver v\n",
    "    ON ROUND(t.trip_longitude,2) = ROUND(v.long,2)\n",
    "    AND ROUND(t.trip_latitude,2) = ROUND(v.lat,2)\n",
    "    AND v.timestamp BETWEEN CAST(t.trip_start_datetime AS DATE) AND CAST(t.trip_end_datetime AS DATE)\n",
    "),\n",
    "trip_with_road_conditions AS (\n",
    "  SELECT\n",
    "    t.*,\n",
    "    r.severity AS road_severity,\n",
    "    r.roadway_name AS road_roadway_name,\n",
    "    r.direction_of_travel AS road_direction_of_travel,\n",
    "    r.description AS road_description,\n",
    "    r.lanes_affected AS road_lanes_affected,\n",
    "    r.lanes_status AS road_lanes_status,\n",
    "    r.navteq_link_id AS road_navteq_link_id,\n",
    "    r.primary_location AS road_primary_location,\n",
    "    r.secondary_location AS road_secondary_location,\n",
    "    r.first_article_city AS road_first_article_city,\n",
    "    r.second_city AS road_second_city,\n",
    "    r.event_type AS road_event_type,\n",
    "    r.event_sub_type AS road_event_sub_type,\n",
    "    r.reported_date AS road_reported_date,\n",
    "    r.start_date AS road_start_date,\n",
    "    r.planned_end_date AS road_planned_end_date\n",
    "  FROM trip_with_volume t\n",
    "  LEFT JOIN {catalog_name}.{schema_name}.road_condition_bronze r\n",
    "    ON ROUND(t.trip_longitude,2) = ROUND(r.longitude,2)\n",
    "    AND ROUND(t.trip_latitude,2) = ROUND(r.latitude,2)\n",
    "    AND t.trip_start_datetime BETWEEN r.start_date AND r.planned_end_date\n",
    "    AND t.trip_end_datetime BETWEEN r.start_date AND r.planned_end_date\n",
    "),\n",
    "trip_with_telematics_tags AS (\n",
    "  SELECT\n",
    "    t.*,\n",
    "    CAST(tag.abs AS INT) AS telematics_abs,\n",
    "    CAST(tag.accelerometer AS INT) AS telematics_accelerometer,\n",
    "    CAST(tag.blind_spot AS INT) AS telematics_blind_spot,\n",
    "    CAST(tag.brake_pad AS INT) AS telematics_brake_pad,\n",
    "    CAST(tag.door_ajar AS INT) AS telematics_door_ajar,\n",
    "    CAST(tag.engine_light AS INT) AS telematics_engine_light,\n",
    "    CAST(tag.fcw AS INT) AS telematics_fcw,\n",
    "    CAST(tag.fog_light AS INT) AS telematics_fog_light,\n",
    "    CAST(tag.lane_departure AS INT) AS telematics_lane_departure,\n",
    "    CAST(tag.parking_sensor AS INT) AS telematics_parking_sensor,\n",
    "    CAST(tag.rain_sensor AS INT) AS telematics_rain_sensor,\n",
    "    CAST(tag.rear_view_camera AS INT) AS telematics_rear_view_camera,\n",
    "    CAST(tag.seatbelt_off AS INT) AS telematics_seatbelt_off,\n",
    "    CAST(tag.tpms AS INT) AS telematics_tpms,\n",
    "    CAST(tag.wiper_baldes AS INT) AS telematics_wiper_blades\n",
    "  FROM trip_with_road_conditions t\n",
    "  LEFT JOIN {catalog_name}.{schema_name}.contributing_factor_tags_lkp_gold tag\n",
    "    ON t.contributing_factor = tag.contributing_factor\n",
    "),\n",
    "final_with_vehicle_id AS (\n",
    "  SELECT\n",
    "    *,\n",
    "    monotonically_increasing_id() AS vehicle_id\n",
    "  FROM trip_with_telematics_tags\n",
    ")\n",
    "SELECT\n",
    "  *,\n",
    "  collision_collision_id IS NOT NULL AS is_collision\n",
    "FROM final_with_vehicle_id;\n",
    "\"\"\"\n",
    "\n",
    "logger.info(sql_query)\n",
    "\n",
    "spark.sql(sql_query)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3545095998802160,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Config",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
