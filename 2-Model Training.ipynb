{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1942d6a5-c96f-41c1-9e8c-ebe5234900de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Geospatial Analytics - Exploration and Model Training\n",
    "\n",
    "**Mobility and Automotive - Road Safety and Risk Prevention**\n",
    "\n",
    "## Overview \n",
    "Trains two ML models using Spark and AutoML to predict collisions/road safety and to forecast traffic volume.\n",
    "\n",
    "The first model is a more elaborate one that uses hyperparameter optimization, while the second leverages AutoML for simplicity and time-to-value.\n",
    "\n",
    "## ML Training and Inference at Scale in Databricks\n",
    "\n",
    "\n",
    "On the first model, we showcase a real-world model with hyperparameter optimization ‚Äî all in Spark, without using Pandas. We also demonstrate how to register the model in Unity Catalog (UC) for governance and perform batch inference using the model loaded from UC. Benefits:\n",
    "\n",
    "- **Performance**: The entire process is built on Spark ‚Äî not on Pandas, which limits processing to a single node (master). This example already includes hyperparameter optimization using Spark (note: Hyperopt is no longer supported on Databricks).\n",
    "- **Simplicity**: The model encapsulates complex logic and accepts a dictionary as input for inference.\n",
    "- **Reusability**: The same transformation logic (dummy creation, handling of null values) is applied both during training and inference.\n",
    "- **Operationalization**: It‚Äôs already integrated with Unity Catalog and MLflow, making it easy to host on Model Serving. Building apps on top of the model is also straightforward.\n",
    "\n",
    "In this notebook, we use pandas for data exploration, as many data scientists are more familiar with pandas for this task. However, this approach does not leverage all Spark nodes/executors. For training and inference, we utilize the full power of Spark.\n",
    "\n",
    "Note that Databricks recommends using either Optuna for single-node optimization or Ray Tune for a distributed experience similar to the now-deprecated Hyperopt.\n",
    "\n",
    "For simplicity, this notebook uses the built-in `TrainValidationSplit` and `ParamGridBuilder` classes from `pyspark.ml.tuning`, allowing cross-validation and hyperparameter optimization directly in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "%pip install urllib3==1.26.14 # support method_whitelist\n",
    "try:\n",
    "    dbutils.library.restartPython()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15c03d1a-11f1-4e6d-8137-214f60157e55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed75cffa-6782-453f-b73a-ae6c28b9bd64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"use catalog {catalog_name}\")\n",
    "spark.sql(f\"use schema {schema_name}\")\n",
    "df = spark.sql(f\"select * from {catalog_name}.{schema_name}.trip_analyics_synthesis_gold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ffc768-3e75-4889-98cc-a621393ef544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Data Exploration\n",
    "\n",
    "### Which variables contribute most to accident?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We use pandas in this section as Data Scientists might be familiar with pandas for data exploration. This approach will not leverage all Spark Nodes/executors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffbd811d-118e-45ce-8b21-fffbefe78279",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Analyzing Numerical attributes"
    }
   },
   "outputs": [],
   "source": [
    "df_pd = df.toPandas()\n",
    "df_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "849fe8cc-0ee7-4ab5-8042-1f2d2156c51c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Analyzing Non Numerical attributes"
    }
   },
   "outputs": [],
   "source": [
    "df_pd.describe(include=[object])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class CorrMatrixCalculator:\n",
    "    def __init__(self,\n",
    "                 df: pd.DataFrame,\n",
    "                 numeric_cols: list[str],\n",
    "                 boolean_cols: list[str],\n",
    "                 categorical_cols: list[str],\n",
    "                 target_col: str = \"is_collision\"):\n",
    "        self.df = df\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.boolean_cols = boolean_cols\n",
    "        self.categorical_cols = categorical_cols\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def _preprocess(self) -> pd.DataFrame:\n",
    "        # 1. slice in only the features + the target\n",
    "        cols = self.numeric_cols + self.boolean_cols + self.categorical_cols + [self.target_col]\n",
    "        df = self.df[cols].copy()\n",
    "\n",
    "        # 2. fill numeric missings\n",
    "        df[self.numeric_cols] = df[self.numeric_cols].fillna(0.0)\n",
    "\n",
    "        # 3. convert booleans to int (0/1) and fill any missings\n",
    "        df[self.boolean_cols + [self.target_col]] = (\n",
    "            df[self.boolean_cols + [self.target_col]]\n",
    "            .fillna(False)\n",
    "            .astype(int)\n",
    "        )\n",
    "\n",
    "        # 4. one-hot encode only the string categoricals\n",
    "        df_enc = pd.get_dummies(\n",
    "            df,\n",
    "            columns=self.categorical_cols,\n",
    "            drop_first=False\n",
    "        )\n",
    "\n",
    "        return df_enc\n",
    "\n",
    "    def correlation_with_target(self) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Returns a sorted series of corr(feature, target), descending\n",
    "        by absolute value.\n",
    "        \"\"\"\n",
    "        df_enc = self._preprocess()\n",
    "        corr = df_enc.corr()[self.target_col].drop(self.target_col)\n",
    "        return corr.abs().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "numeric_cols = [\n",
    "    \"weather_temperature_2m\", \"weather_precipitation\", \"weather_rain\",\n",
    "    \"weather_snowfall\", \"weather_snow_depth\", \"weather_weather_code\",\n",
    "    \"weather_wind_speed_10m\", \"weather_wind_direction_10m\",\n",
    "    \"weather_wind_gusts_10m\", \"traffic_volume\", \"trip_h3_index\"\n",
    "]\n",
    "\n",
    "boolean_cols = [\n",
    "    \"telematics_abs\", \"telematics_accelerometer\", \"telematics_blind_spot\",\n",
    "    \"telematics_brake_pad\", \"telematics_door_ajar\", \"telematics_engine_light\",\n",
    "    \"telematics_fcw\", \"telematics_fog_light\", \"telematics_lane_departure\",\n",
    "    \"telematics_parking_sensor\", \"telematics_rain_sensor\",\n",
    "    \"telematics_rear_view_camera\", \"telematics_seatbelt_off\",\n",
    "    \"telematics_tpms\", \"telematics_wiper_blades\"\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    \"road_severity\", \"road_event_type\", \"road_event_sub_type\"\n",
    "]\n",
    "\n",
    "# assume df_pd is your pandas DataFrame\n",
    "calc = CorrMatrixCalculator(df_pd, numeric_cols, boolean_cols, categorical_cols)\n",
    "feature_corr = calc.correlation_with_target()\n",
    "\n",
    "print(\"Features ranked by |corr| with is_collision:\")\n",
    "display(feature_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights üí°üí°\n",
    "\n",
    "A correlation coefficient of 0.226 for traffic volume indicates a mild positive linear relationship with collisions. Because it‚Äôs only 0.226 (meaning traffic volume alone accounts for just 5 % of the ups and downs in crash), we know most of the story isn‚Äôt just ‚Äúhow many cars are on the road.‚Äù Traffic volume is one piece of a much bigger puzzle.\n",
    "\n",
    "In our case, our synthetically generated telematics data act as a proxy for driving behavior and shows a stronger correlation with collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "corr_df = feature_corr.to_frame(name=\"abs_corr\")\n",
    "\n",
    "#Plot with seaborn heatmap\n",
    "plt.figure(figsize=(4, len(corr_df) * 0.3))\n",
    "sns.heatmap(\n",
    "    corr_df,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    cbar_kws={\"label\": \"Absolute Correlation\"}\n",
    ")\n",
    "plt.xlabel(\"\")           # no x-axis label needed\n",
    "plt.ylabel(\"Features\")\n",
    "plt.yticks(rotation=0)    # keep feature names horizontal\n",
    "plt.title(\"Feature Correlation with is_collision\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df_enc = calc._preprocess()\n",
    "\n",
    "corr_matrix = df_enc.corr()\n",
    "labels = corr_matrix.columns.tolist()\n",
    "n = len(labels)\n",
    "\n",
    "# 3. Plot with matplotlib\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "im = ax.imshow(corr_matrix.values, aspect='auto', cmap='coolwarm')\n",
    "fig.colorbar(im)\n",
    "\n",
    "# set ticks\n",
    "ax.set_xticks(range(n))\n",
    "ax.set_yticks(range(n))\n",
    "ax.set_xticklabels(labels, rotation=90)\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "# highlight \"is_collision\" tick labels in red\n",
    "for lbl in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "    if lbl.get_text() == \"is_collision\":\n",
    "        lbl.set_color(\"blue\")\n",
    "        lbl.set_weight(\"bold\")\n",
    "        lbl.set_size(12)\n",
    "\n",
    "ax.set_title(\"Feature Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Volume Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keplergl import KeplerGl\n",
    "from IPython.display import display\n",
    "\n",
    "df_traffic = spark.read.table(f\"{catalog_name}.{schema_name}.traffic_volume_counts_silver\")\n",
    "\n",
    "# Step 2: Aggregate and filter traffic volume\n",
    "df_dense = (\n",
    "    df_traffic.groupBy(\"h3_index\", \"lat\", \"long\")\n",
    "    .agg({\"vol\": \"avg\"})\n",
    "    .withColumnRenamed(\"avg(vol)\", \"traffic_volume\")\n",
    "    .filter(\"traffic_volume > 50\")  # adjust threshold as needed\n",
    ")\n",
    "\n",
    "# Step 3: Add hex string for Kepler\n",
    "df_hex_vis = df_dense.selectExpr(\n",
    "    \"h3_h3tostring(h3_index) as hex_id\",\n",
    "    \"lat\", \"long\", \"traffic_volume\"\n",
    ")\n",
    "\n",
    "# Step 4: Compute center for map\n",
    "center = df_hex_vis.selectExpr(\"avg(lat) as lat\", \"avg(long) as lon\").first()\n",
    "center_lat = center[\"lat\"]\n",
    "center_lon = center[\"lon\"]\n",
    "\n",
    "# Step 5: Configure Kepler map centered on your data\n",
    "map_config = {\n",
    "    \"version\": \"v1\",\n",
    "    \"config\": {\n",
    "        \"mapState\": {\n",
    "            \"latitude\": center_lat,\n",
    "            \"longitude\": center_lon,\n",
    "            \"zoom\": 10,\n",
    "            \"pitch\": 0,\n",
    "            \"bearing\": 0\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Step 6: Plot the data\n",
    "map_traffic = KeplerGl(height=600, config=map_config)\n",
    "map_traffic.add_data(\n",
    "    data = df_hex_vis.toPandas(),\n",
    "    name = \"Traffic Volume by H3\"\n",
    ")\n",
    "\n",
    "# Step 7: Display\n",
    "display(map_traffic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e319a04-0426-4dc9-9b67-252d228dca1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model 1: Predict Collisions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pipeline Model to Handle Training & Inference Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f68f52c-5cae-4e59-aafa-cb9d1a2cbb1f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get the Best Model URI"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "numeric_cols = [\n",
    "    \"weather_temperature_2m\", \"weather_precipitation\", \"weather_rain\",\n",
    "    \"weather_snowfall\", \"weather_snow_depth\", \"weather_weather_code\",\n",
    "    \"weather_wind_speed_10m\", \"weather_wind_direction_10m\",\n",
    "    \"weather_wind_gusts_10m\", \"traffic_volume\", \"trip_h3_index\"\n",
    "]\n",
    "\n",
    "boolean_cols = [\n",
    "    \"telematics_abs\", \"telematics_accelerometer\", \"telematics_blind_spot\",\n",
    "    \"telematics_brake_pad\", \"telematics_door_ajar\", \"telematics_engine_light\",\n",
    "    \"telematics_fcw\", \"telematics_fog_light\", \"telematics_lane_departure\",\n",
    "    \"telematics_parking_sensor\", \"telematics_rain_sensor\",\n",
    "    \"telematics_rear_view_camera\", \"telematics_seatbelt_off\",\n",
    "    \"telematics_tpms\", \"telematics_wiper_blades\"\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    \"road_severity\", \"road_event_type\", \"road_event_sub_type\"\n",
    "]\n",
    "\n",
    "df = spark.sql(f\"select * from {catalog_name}.{schema_name}.`trip_analyics_synthesis_gold`\")\n",
    "\n",
    "##### \n",
    "# Select only the columns we care about + target\n",
    "selected_cols = numeric_cols + boolean_cols + categorical_cols + [\"is_collision\"]\n",
    "df_sel = df.select(*selected_cols)\n",
    "\n",
    "# Fill missing numerics with 0.0\n",
    "fill_map = { c: 0.0 for c in numeric_cols }\n",
    "df_sel = df_sel.fillna(fill_map)\n",
    "\n",
    "# Cast booleans (and your target) to 0/1 ints\n",
    "for b in boolean_cols + [\"is_collision\"]:\n",
    "    df_sel = df_sel.withColumn(b, F.when(F.col(b) == True, 1).otherwise(0))\n",
    "\n",
    "# Index + one-hot encode your string categoricals\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=col + \"_idx\", handleInvalid=\"keep\")\n",
    "    for col in categorical_cols\n",
    "]\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=col + \"_idx\", outputCol=col + \"_ohe\")\n",
    "    for col in categorical_cols\n",
    "]\n",
    "\n",
    "# Assemble everything into a single features vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = numeric_cols\n",
    "              + boolean_cols\n",
    "              + [c + \"_ohe\" for c in categorical_cols],\n",
    "    outputCol = \"features\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages = indexers + encoders + [assembler])\n",
    "pipeline_model    = pipeline.fit(df_sel)\n",
    "train_df = pipeline_model.transform(df_sel).select(\"features\", \"is_collision\")\n",
    "\n",
    "input_cols = {\n",
    "    \"numeric\": numeric_cols,\n",
    "    \"boolean\": boolean_cols,\n",
    "    \"categorical\": categorical_cols\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Wrapper Model to simplify inference signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec, DataType\n",
    "\n",
    "class CollisionRiskModel(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, input_cols):\n",
    "        self.input_cols = input_cols  # ensure this is a plain dict\n",
    "\n",
    "    def load_context(self, context):\n",
    "        import mlflow.spark\n",
    "        from pyspark.sql import functions as F\n",
    "        self.F = F\n",
    "        self.pipeline_model = mlflow.spark.load_model(context.artifacts[\"pipeline_model\"])\n",
    "        self.gbt_model = mlflow.spark.load_model(context.artifacts[\"gbt_model\"])\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        from pyspark.sql import SparkSession\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "        spark_input = spark.createDataFrame(model_input)\n",
    "        spark_input = spark_input.fillna({col: 0.0 for col in self.input_cols['numeric']})\n",
    "\n",
    "        for b in self.input_cols['boolean']:\n",
    "            spark_input = spark_input.withColumn(b, self.F.when(self.F.col(b) == True, 1).otherwise(0))\n",
    "\n",
    "        transformed = self.pipeline_model.transform(spark_input)\n",
    "        result = self.gbt_model.transform(transformed)\n",
    "        result = result.withColumn(\"probability\", self.F.col(\"probability\").cast(\"string\"))\n",
    "\n",
    "        return result.select(\"prediction\", \"probability\").toPandas()\n",
    "      \n",
    "    @staticmethod\n",
    "    def get_signature():\n",
    "        input_schema = Schema([\n",
    "            ColSpec(DataType.double, \"weather_temperature_2m\"),\n",
    "            ColSpec(DataType.double, \"weather_precipitation\"),\n",
    "            ColSpec(DataType.double, \"weather_rain\"),\n",
    "            ColSpec(DataType.double, \"weather_snowfall\"),\n",
    "            ColSpec(DataType.double, \"weather_snow_depth\"),\n",
    "            ColSpec(DataType.double, \"weather_weather_code\"),\n",
    "            ColSpec(DataType.double, \"weather_wind_speed_10m\"),\n",
    "            ColSpec(DataType.double, \"weather_wind_direction_10m\"),\n",
    "            ColSpec(DataType.double, \"weather_wind_gusts_10m\"),\n",
    "            ColSpec(DataType.double, \"traffic_volume\"),\n",
    "            ColSpec(DataType.long, \"trip_h3_index\"),\n",
    "            ColSpec(DataType.integer, \"telematics_abs\"),\n",
    "            ColSpec(DataType.integer, \"telematics_accelerometer\"),\n",
    "            ColSpec(DataType.integer, \"telematics_blind_spot\"),\n",
    "            ColSpec(DataType.integer, \"telematics_brake_pad\"),\n",
    "            ColSpec(DataType.integer, \"telematics_door_ajar\"),\n",
    "            ColSpec(DataType.integer, \"telematics_engine_light\"),\n",
    "            ColSpec(DataType.integer, \"telematics_fcw\"),\n",
    "            ColSpec(DataType.integer, \"telematics_fog_light\"),\n",
    "            ColSpec(DataType.integer, \"telematics_lane_departure\"),\n",
    "            ColSpec(DataType.integer, \"telematics_parking_sensor\"),\n",
    "            ColSpec(DataType.integer, \"telematics_rain_sensor\"),\n",
    "            ColSpec(DataType.integer, \"telematics_rear_view_camera\"),\n",
    "            ColSpec(DataType.integer, \"telematics_seatbelt_off\"),\n",
    "            ColSpec(DataType.integer, \"telematics_tpms\"),\n",
    "            ColSpec(DataType.integer, \"telematics_wiper_blades\"),\n",
    "            ColSpec(DataType.string, \"road_severity\"),\n",
    "            ColSpec(DataType.string, \"road_event_type\"),\n",
    "            ColSpec(DataType.string, \"road_event_sub_type\"),\n",
    "        ])\n",
    "\n",
    "        output_schema = Schema([\n",
    "            ColSpec(DataType.double, \"prediction\"),\n",
    "            ColSpec(DataType.string, \"probability\"),  # workaround: serialize probability vector as string\n",
    "        ])\n",
    "\n",
    "        return ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_sample():\n",
    "        # Sample input dictionary\n",
    "        sample_input = {\n",
    "            \"weather_temperature_2m\": 22.0,\n",
    "            \"weather_precipitation\": 0.0,\n",
    "            \"weather_rain\": 0.0,\n",
    "            \"weather_snowfall\": 0.0,\n",
    "            \"weather_snow_depth\": 0.0,\n",
    "            \"weather_weather_code\": 3.0,\n",
    "            \"weather_wind_speed_10m\": 5.2,\n",
    "            \"weather_wind_direction_10m\": 180.0,\n",
    "            \"weather_wind_gusts_10m\": 7.0,\n",
    "            \"traffic_volume\": 1200.0,\n",
    "            \"trip_h3_index\": 622236172821782527,\n",
    "            \"telematics_abs\": 1,\n",
    "            \"telematics_accelerometer\": 1,\n",
    "            \"telematics_blind_spot\": 0,\n",
    "            \"telematics_brake_pad\": 1,\n",
    "            \"telematics_door_ajar\": 0,\n",
    "            \"telematics_engine_light\": 0,\n",
    "            \"telematics_fcw\": 1,\n",
    "            \"telematics_fog_light\": 0,\n",
    "            \"telematics_lane_departure\": 1,\n",
    "            \"telematics_parking_sensor\": 0,\n",
    "            \"telematics_rain_sensor\": 0,\n",
    "            \"telematics_rear_view_camera\": 1,\n",
    "            \"telematics_seatbelt_off\": 0,\n",
    "            \"telematics_tpms\": 0,\n",
    "            \"telematics_wiper_blades\": 0,\n",
    "            \"road_severity\": \"Moderate\",\n",
    "            \"road_event_type\": \"Accident\",\n",
    "            \"road_event_sub_type\": \"Rear-End\"\n",
    "        }\n",
    "\n",
    "        return sample_input\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model and Register in UC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Handle class imbalance\n",
    "# -----------------------------\n",
    "\n",
    "# Count positives and negatives\n",
    "pos_count = train_df.filter(\"is_collision = 1\").count()\n",
    "neg_count = train_df.filter(\"is_collision = 0\").count()\n",
    "\n",
    "# Compute weights\n",
    "majority_weight = 1.0\n",
    "minority_weight = float(neg_count) / float(pos_count)\n",
    "\n",
    "# Add class_weight column\n",
    "train_df = train_df.withColumn(\n",
    "    \"class_weight\",\n",
    "    F.when(F.col(\"is_collision\") == 1, minority_weight).otherwise(majority_weight)\n",
    ")\n",
    "\n",
    "# Split into training/validation sets\n",
    "train, val = train_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Train GBT model with tuning\n",
    "# -----------------------------\n",
    "\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"is_collision\",\n",
    "    weightCol=\"class_weight\"\n",
    ")\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [3, 5, 7])\n",
    "             .addGrid(gbt.maxIter, [20, 50, 100])\n",
    "             .build())\n",
    "\n",
    "auc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"is_collision\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"is_collision\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=gbt,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=auc_evaluator,\n",
    "    trainRatio=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Step 3: MLflow training and registration\n",
    "# -----------------------------\n",
    "\n",
    "#mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "    # Train model\n",
    "    model = tvs.fit(train)\n",
    "    best_model = model.bestModel\n",
    "\n",
    "    # Evaluate predictions\n",
    "    predictions = best_model.transform(val)\n",
    "    predictions = predictions.withColumn(\"label\", F.col(\"is_collision\"))\n",
    "\n",
    "    # AUC\n",
    "    auc = auc_evaluator.evaluate(predictions)\n",
    "    mlflow.log_metric(\"val_auc\", auc)\n",
    "\n",
    "    # F1 Score using Spark (2 * (precision * recall) / (precision + recall))\n",
    "    val_predictions = best_model.transform(val)\n",
    "\n",
    "    f1_score = f1_evaluator.evaluate(val_predictions)\n",
    "\n",
    "    mlflow.log_metric(\"val_f1\", f1_score)\n",
    "\n",
    "    artifact_path = \"custom_model\"\n",
    "\n",
    "    pipeline_model_path = f\"{model_path}/collision_pipeline\"\n",
    "    gbt_model_path = f\"{model_path}/collision_gbt\"\n",
    "    \n",
    "    mlflow.spark.save_model(pipeline_model, pipeline_model_path)\n",
    "    mlflow.spark.save_model(best_model, gbt_model_path)\n",
    "\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=artifact_path,\n",
    "        artifacts={\n",
    "            \"pipeline_model\": f\"{model_path}/collision_pipeline\",\n",
    "            \"gbt_model\": f\"{model_path}/collision_gbt\",\n",
    "        },\n",
    "        python_model=CollisionRiskModel(\n",
    "            input_cols=input_cols\n",
    "        ),\n",
    "        input_example=[CollisionRiskModel.get_sample()],\n",
    "        signature=CollisionRiskModel.get_signature()\n",
    "    )\n",
    "\n",
    "    model_uri = f\"runs:/{run_id}/{artifact_path}\"\n",
    "    mv = mlflow.register_model(model_uri, model_name)\n",
    "\n",
    "print(f\"‚úÖ AUC: {auc:.4f} | F1: {f1_score:.4f}\")\n",
    "print(f\"‚úÖ Model registered as: {model_name}\")\n",
    "print(f\"üìå Model URI: {model_uri}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test prediction on the registered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "#model_name = \"auto_geospatial.default_v3.collision_prediction\"\n",
    "\n",
    "sample_input = {\n",
    "    \"weather_temperature_2m\": 22.0,\n",
    "    \"weather_precipitation\": 0.0,\n",
    "    \"weather_rain\": 0.0,\n",
    "    \"weather_snowfall\": 0.0,\n",
    "    \"weather_snow_depth\": 0.0,\n",
    "    \"weather_weather_code\": 3.0,\n",
    "    \"weather_wind_speed_10m\": 5.2,\n",
    "    \"weather_wind_direction_10m\": 180.0,\n",
    "    \"weather_wind_gusts_10m\": 7.0,\n",
    "    \"traffic_volume\": 1200.0,\n",
    "    \"trip_h3_index\": 622236172821782527,\n",
    "    \"telematics_abs\": 1,\n",
    "    \"telematics_accelerometer\": 1,\n",
    "    \"telematics_blind_spot\": 0,\n",
    "    \"telematics_brake_pad\": 1,\n",
    "    \"telematics_door_ajar\": 0,\n",
    "    \"telematics_engine_light\": 0,\n",
    "    \"telematics_fcw\": 1,\n",
    "    \"telematics_fog_light\": 0,\n",
    "    \"telematics_lane_departure\": 1,\n",
    "    \"telematics_parking_sensor\": 0,\n",
    "    \"telematics_rain_sensor\": 0,\n",
    "    \"telematics_rear_view_camera\": 1,\n",
    "    \"telematics_seatbelt_off\": 0,\n",
    "    \"telematics_tpms\": 0,\n",
    "    \"telematics_wiper_blades\": 0,\n",
    "    \"road_severity\": \"Moderate\",\n",
    "    \"road_event_type\": \"Accident\",\n",
    "    \"road_event_sub_type\": \"Rear-End\"\n",
    "}\n",
    "\n",
    "# Load the model from UC\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# Convert sample input to DataFrame\n",
    "pd_df = pd.DataFrame([sample_input])\n",
    "pd_df = pd_df.astype({col: 'int32' for col in pd_df.select_dtypes(include=['int64', 'int']).columns})\n",
    "\n",
    "predictions = loaded_model.predict(pd_df)\n",
    "\n",
    "display(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Forecast Traffic Volume\n",
    "\n",
    "In this section, we train a basic forecasting model for predicting Traffic Volume using [Databricks AutoML (classic compute)](https://docs.databricks.com/aws/en/machine-learning/automl/forecasting).\n",
    "\n",
    "The following command starts an AutoML run. You must provide the column that the model should predict in the `target_col` argument and the time column.  \n",
    "When the run completes, you can follow the link to the best trial notebook to examine the training code.\n",
    "\n",
    "This example also specifies:\n",
    "- `horizon=90` to indicate that AutoML should forecast 90 days into the future. \n",
    "- `frequency=\"d\"` to specify that a forecast should be provided for each month. \n",
    "- `primary_metric=\"mdape\"` to specify the metric to optimize for during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traing with AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import databricks.automl\n",
    "import mlflow.pyfunc\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import logging\n",
    " \n",
    "# Disable informational messages \n",
    "logging.getLogger(\"py4j\").setLevel(logging.WARNING)\n",
    "\n",
    "# Forecast Traffic Volume with AutoML \n",
    "summary = databricks.automl.forecast(\n",
    "    dataset=df_traffic,\n",
    "    time_col=\"timestamp\",\n",
    "    target_col=\"vol\",\n",
    "    timeout_minutes=60,\n",
    "    horizon=90, \n",
    "    frequency=\"d\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = MlflowClient()\n",
    "\n",
    "trial_id = summary.best_trial.mlflow_run_id\n",
    "\n",
    "model_uri = f\"runs:/{trial_id}/model\"\n",
    "pyfunc_model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "\n",
    "forecasts = pyfunc_model._model_impl.python_model.predict_timeseries()\n",
    "display(forecasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Forecast with Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_true = df_traffic.groupby(\"timestamp\").agg({\"vol\": \"avg\"}).toPandas().reset_index()\n",
    "\n",
    "# Plot the forecasted points\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(facecolor='w', figsize=(10, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "forecasts = pyfunc_model._model_impl.python_model.predict_timeseries(include_history=True)\n",
    "forecasts = forecasts[(forecasts['ds'].dt.year >= 2024) & (forecasts['ds'].dt.year <= 2025)]\n",
    "fcst_t = forecasts['ds'].dt.to_pydatetime()\n",
    "ax.plot(df_true['timestamp'].dt.to_pydatetime(), df_true['avg(vol)'], 'k.', label='Observed data points')\n",
    "ax.plot(fcst_t, forecasts['yhat'], ls='-', c='#0072B2', label='Forecasts')\n",
    "ax.fill_between(fcst_t, forecasts['yhat_lower'], forecasts['yhat_upper'],\n",
    "                color='#0072B2', alpha=0.2, label='Uncertainty interval')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1436459421785398,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "2. Model Training",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
